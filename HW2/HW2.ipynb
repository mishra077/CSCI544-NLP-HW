{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699be8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mishr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "import warnings\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import argmax\n",
    "from copy import deepcopy\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# torch.cuda.set_device(device)\n",
    "warnings.filterwarnings('ignore')\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed2ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTranformation(object):\n",
    "\n",
    "    def __init__(self, filename, preprocess):\n",
    "        self.filename = filename\n",
    "        self.random_state = 10\n",
    "        self.n = 50000\n",
    "        self.preprocess = preprocess\n",
    "        print(\"Preproces: \" + str(preprocess))\n",
    "\n",
    "    def read_file(self, error_bad_lines = False, warn_bad_lines = False, sep = \"\\t\"):\n",
    "        df = pd.read_csv(self.filename, sep = sep, error_bad_lines = error_bad_lines, warn_bad_lines = warn_bad_lines)\n",
    "        df = df.dropna()\n",
    "        return df\n",
    "    \n",
    "    def formation(self, row1 = 'review_body', row2 = 'star_rating', ):\n",
    "        df = self.read_file()\n",
    "        df = df[[row1, row2]]\n",
    "        df = df.dropna()\n",
    "\n",
    "        dataset = pd.concat([df[df['star_rating'] == 1].sample(n = 50000, random_state = 10), \n",
    "                    df[df['star_rating'] == 2].sample(n = 50000, random_state = 10),\n",
    "                    df[df['star_rating'] == 3].sample(n = 50000, random_state = 10),\n",
    "                    df[df['star_rating'] == 4].sample(n = 50000, random_state = 10),\n",
    "                    df[df['star_rating'] == 5].sample(n = 50000, random_state = 10)])\n",
    "\n",
    "        dataset = dataset.reset_index(drop = True)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def label(self, rows):\n",
    "        if rows.star_rating > 3:\n",
    "            return 1\n",
    "        elif rows.star_rating < 3:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    def apply_label(self):\n",
    "        dataset = self.formation()\n",
    "        dataset['label'] = dataset.apply(lambda row : self.label(row), axis = 1)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def remove_html_and_url(self, s):\n",
    "        a = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', s, flags=re.MULTILINE)\n",
    "        soup = BeautifulSoup(a, 'html.parser')\n",
    "        a = soup.get_text()\n",
    "        return a\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        text_tokens = word_tokenize(s)\n",
    "        return text_tokens\n",
    "\n",
    "    def without_preprocess(self):\n",
    "        dataset = self.apply_label()\n",
    "        dataset.review_body = dataset.review_body.apply(self.tokenize)\n",
    "        return dataset\n",
    "    \n",
    "    def with_preprocess(self):\n",
    "        dataset = self.apply_label()\n",
    "        dataset.review_body = dataset.review_body.str.lower()\n",
    "\n",
    "        dataset.review_body = dataset.review_body.apply(lambda s: self.remove_html_and_url(s))\n",
    "        dataset.review_body = dataset.review_body.apply(lambda s: re.sub(\"[^a-zA-Z']+\", \" \", s))\n",
    "        dataset.review_body = dataset.review_body.apply(lambda s: re.sub(' +', ' ', s))\n",
    "\n",
    "        dataset.review_body = dataset.review_body.apply(self.tokenize)\n",
    "\n",
    "        dataset.dropna()\n",
    "        return dataset\n",
    "\n",
    "    def train_test_split(self):\n",
    "\n",
    "        if self.preprocess:\n",
    "            dataset = self.with_preprocess()\n",
    "        else:\n",
    "            dataset = self.without_preprocess()\n",
    "\n",
    "        train = dataset.sample(frac = 0.8, random_state = 200)\n",
    "        test = dataset.drop(train.index)\n",
    "        train = train.reset_index(drop = True)\n",
    "        test = test.reset_index(drop = True)\n",
    "\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b41b3",
   "metadata": {},
   "source": [
    "Here I made a class for data processing and splitting the dataset into train and test dataset.\n",
    "So for constructor __init__ method will take filename from where we have to read the files, preprocess is a boolean variable which tells if we want to process the dataset. By processing the dataset means to clean the dataset (removing html and urls markups, removing non-alphabetic characters, converting all the strings to lowercase).\n",
    "\n",
    "Methods:\n",
    "read_file:\n",
    "    args: error_bad_lines -> boolean {to remove all the lines if they mismatch the size of the dataframe}\n",
    "          warn_bad_lines -> boolean {to remove the warning if while reading csv file we found any null cells or bad rows or columns}\n",
    "          sep {which sperator is used by the file. For eg csv files are seperated by ',', in tsv files the rows are sepearted by tabs '/t'}\n",
    "    Objective: to read the file using pandas and dropping all the rows if it contains null or empty cells.\n",
    "    Return: dataframe\n",
    "\n",
    "formation:\n",
    "    args: row1, row2 -> string {mentioning which columns are required for training}\n",
    "    Objective: as the question suggested we need to 50000 samples for every star rating so in this method we just store random 50k samples for each star rating and push it into new dataframe.\n",
    "    Return: dataframe\n",
    "\n",
    "label:\n",
    "    args: None\n",
    "    Objective : a method to map the star ratings to a particular class label based on the conditions given in the question\n",
    "                Condition: if star rating is greater than 3 then map it to class label '1'.\n",
    "                           if star rating is less than 3 then map it to class label '2'.\n",
    "                           else all the star rating equal to 3 map it to class label '3'.\n",
    "    Return: None\n",
    "\n",
    "apply_label:\n",
    "    args: None\n",
    "    Objective: to apply above method onto the dataframe generated from formation method.\n",
    "    Return: dataframe\n",
    "\n",
    "remove_html_and_url:\n",
    "    args: s -> string\n",
    "    Objective: to remove all the html markups and url from a given string\n",
    "    Return: string\n",
    "\n",
    "tokenize:\n",
    "    args: s -> string\n",
    "    Objective: to tokenize the string (Converting a given string into list of words or tokens)\n",
    "    Return: List\n",
    "\n",
    "without_preprocess:\n",
    "    args: None\n",
    "    Objective: To return the dataframe without preprocessing the dataset.\n",
    "    Return: dataframe\n",
    "\n",
    "with_preprocess:\n",
    "    args: None\n",
    "    Objective: To return the dataframe with preprocessed data. We will remove all the html, urls, extra spaces and non-alphabetic characters from the dataset.\n",
    "    Return: dataframe\n",
    "\n",
    "train_test_split:\n",
    "    args: None\n",
    "    Objective: to split the dataframe into training and testing dataframe. We are using 80-20 split over here.\n",
    "    Return: dataframe                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca7c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorization(object):\n",
    "\n",
    "    def __init__(self, model, dataset, model_type = \"model\", classification = \"binary\", mode = \"mean\", pad = False):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.model_type = model_type # our own model or pretrained\n",
    "        self.classification = classification # binary or multi-class\n",
    "        if self.model_type == \"pretrained\":\n",
    "            self.vocab = self.model\n",
    "        if self.model_type == \"model\":\n",
    "            self.vocab = self.model.wv\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.pad =pad\n",
    "\n",
    "        print(\"Vectorizing training dataset....\")\n",
    "        print(\"Model Type: \" + self.model_type)\n",
    "        print(\"Classification: \" + self.classification)\n",
    "\n",
    "    def get_mean_vector(self, data_review_body, data_label):\n",
    "\n",
    "        if self.classification == \"binary\":\n",
    "            if data_label != 3:\n",
    "                if self.model_type == \"model\":\n",
    "                    words = [word for word in data_review_body if word in self.vocab.index_to_key]\n",
    "                    if len(words) >= 1:\n",
    "                        rev = []\n",
    "                        for word in words:\n",
    "                            rev.append(np.array(self.vocab[word]))\n",
    "                            \n",
    "                        if type(data_label) is not int : print(\"Found\")\n",
    "                        return rev, data_label\n",
    "                else:\n",
    "                    words = [word for word in data_review_body if word in self.vocab]\n",
    "                    if len(words) >= 1:\n",
    "                        rev = []\n",
    "                        for word in words:\n",
    "                            rev.append(np.array(self.vocab[word]))\n",
    "                            \n",
    "                        if type(data_label) is not int : print(\"Found\")\n",
    "                        return rev, data_label\n",
    "\n",
    "        else:\n",
    "            if self.model_type == \"mode\":\n",
    "                words = [word for word in data_review_body if word in self.vocab.index_to_key]\n",
    "                if len(words) >= 1:\n",
    "                    rev = []\n",
    "                    for word in words:\n",
    "                        rev.append(np.array(self.vocab[word]))\n",
    "                    return rev, data_label\n",
    "            else:\n",
    "                words = [word for word in data_review_body if word in self.vocab]\n",
    "                if len(words) >= 1:\n",
    "                    rev = []\n",
    "                    for word in words:\n",
    "                        rev.append(np.array(self.vocab[word]))\n",
    "                    return rev, data_label\n",
    "\n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        feature = []\n",
    "        y_label = []\n",
    "        # print(self.vocab.index_to_key)\n",
    "        for data_review_body, data_label in zip(self.dataset.review_body, self.dataset.label):\n",
    "            try:\n",
    "                x, y = self.get_mean_vector(data_review_body, data_label)\n",
    "                if self.pad:\n",
    "                    if len(x) >= 50:\n",
    "                        feature.append(x[:50])\n",
    "                        y_label.append(y)\n",
    "                    else:\n",
    "                        feature.append(x)\n",
    "                        y_label.append(y)\n",
    "                else:\n",
    "                    if self.mode == \"vec\":\n",
    "                        if len(x) >= 10:\n",
    "                            feature.append(x[:10])\n",
    "                            y_label.append(y)\n",
    "                    else:    \n",
    "                        feature.append(np.mean(x, axis = 0))\n",
    "                        y_label.append(y)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"Vectorization Completed\")\n",
    "        return feature, y_label\n",
    "    \n",
    "    def pad_review(self, review, seq_len):\n",
    "    \n",
    "        features = np.zeros((seq_len, 300), dtype=float)\n",
    "        features[-len(review):] = np.array(review)[:seq_len]\n",
    "    \n",
    "        return features\n",
    "\n",
    "    def join_words(self, x):\n",
    "        y = \"\"\n",
    "        for ele in x:\n",
    "            y = ' '.join(ele)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098bbb5",
   "metadata": {},
   "source": [
    "In this class we generate training and testing dataset for different model.\n",
    "Constructor take several arguments:\n",
    "    model: Either our general model or pretrained model\n",
    "    dataset: training or testing\n",
    "    model_type: Either our general model or pretrained model -> string\n",
    "    classification: binary or multiclass. -> string\n",
    "    mode: mean or vectors (Given in the question we need to either form mean model which is simple model or to generate 10-vec/50-vec dataset for MLP, RNN, and GRU) -> string\n",
    "    pad: So in the given question for RNN and GRU we need to padding for the dataset whose vector size is less than 50. -> boolean\n",
    "\n",
    "Methods:\n",
    "get_mean_vector:\n",
    "    args: data_review_body, data_label: dataframes column\n",
    "    Objective: extracting the feature vectors for calcualting mean for specific models\n",
    "    Return: corresponding feature data vectors and its label\n",
    "\n",
    "feature_extraction:\n",
    "    args: None\n",
    "    Objective: Based on the cases it generates the dataset. For eg for a simple binary model it will generate mean vectors for each row of data review body, for rnn it will generate 50 x 300 feature  vector.\n",
    "    Return: List of features and label\n",
    "\n",
    "pad_review:\n",
    "    args: review, seq_len : review -> list of words for a particualr row, seq_len-> len of the list\n",
    "    Objective: This method is for RNN and GRU data generation. As the question stated we need to pad zeros in feature map which have seq_len less than 50.\n",
    "    Return: feature map\n",
    "\n",
    "join_words:\n",
    "    args: x->list of words\n",
    "    Objective: to convert list of words into string\n",
    "    Return: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fc0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    def __iter__(self):\n",
    "        for row in self.dataset:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5935cc",
   "metadata": {},
   "source": [
    "Sentence class is an iterator class which yields single row of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79de28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Percept(object):\n",
    "\n",
    "    def __init__(self, X_train, Y_train, X_test, Y_test, max_iter = 100, random_state = 20, eta0 = 0.01, verbose = 1):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.eta0 = eta0\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def metrics(self, true, pred):\n",
    "        tn, fp, fn, tp = cm(true, pred).ravel()\n",
    "        acc = (tp + tn)/(tn + fp + fn + tp)\n",
    "        prec = tp/(tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "        f1 = 2*(rec * prec) / (rec + prec)\n",
    "        return [acc, prec, rec, f1]\n",
    "\n",
    "    def print_seq(self, score_list):\n",
    "        print(\"%.6f\" % score_list[0], \"\\n%.6f\" % score_list[1], \"\\n%.6f\" % score_list[2], \"\\n%.6f\" % score_list[3])\n",
    "\n",
    "    def perceptron_model(self):\n",
    "        percept = Perceptron(max_iter = self.max_iter, random_state = self.random_state, eta0 = self.eta0, verbose=self.verbose)\n",
    "\n",
    "        print(\"Fitting the Model...\")\n",
    "        percept.fit(self.X_train, self.Y_train)\n",
    "        return percept\n",
    "\n",
    "    def evaluation(self):\n",
    "        percept = self.perceptron_model()\n",
    "\n",
    "        print(\"Evaluating the model on training dataset..\")\n",
    "        Y_train_pred = percept.predict(self.X_train)\n",
    "        \n",
    "        train_score = self.metrics(self.Y_train, Y_train_pred)\n",
    "\n",
    "        print(\"Evaluating the model on testing dataset...\")\n",
    "        Y_test_pred = percept.predict(self.X_test)\n",
    "        test_score = self.metrics(self.Y_test, Y_test_pred)\n",
    "\n",
    "        print(\"Training Score\")\n",
    "        self.print_seq(train_score)\n",
    "\n",
    "        print(\"Testing Score\")\n",
    "        self.print_seq(test_score)\n",
    "        \n",
    "        return test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7acdd6",
   "metadata": {},
   "source": [
    "Perceptron class\n",
    "Contructor takes dataset, number of epochs, random_state, learning rate and verbose as argument\n",
    "This class is used for training preceptron for different model type and mode of classification\n",
    "\n",
    "Methods:\n",
    "metrics:\n",
    "    args: true, pred {true means true label and pred means predicted label by perceptron}\n",
    "    Objective: To calculate accuracy, precision, recall and F-score with the help of confusion matrix\n",
    "    Return: list of metrics\n",
    "    \n",
    "print_seq:\n",
    "    args: list of metrics\n",
    "    Objective: printing the score\n",
    "    Return: None\n",
    "\n",
    "preceptron_model:\n",
    "    args: None\n",
    "    Objective: intitalizing preceptron model and fitting the model on training dataset\n",
    "    Return: preceptron model\n",
    "\n",
    "Evaluation:\n",
    "    args: None\n",
    "    Objective: predicting the preceptron model on testing dataset and calulating the performance of the trained perceptron model\n",
    "    Return: Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80630a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, X_train, Y_train, X_test, Y_test, max_iter = 500):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def intitalize_model(self):\n",
    "        # Linear SVM\n",
    "        svc = SVC(max_iter = self.max_iter)\n",
    "        \n",
    "        print(\"Fitting the SVM\")\n",
    "        svc_model = svc.fit(self.X_train, self.Y_train)\n",
    "        return svc_model\n",
    "\n",
    "    def print_seq(self, score_list):\n",
    "        print(\"%.6f\" % score_list[0], \"\\n%.6f\" % score_list[1], \"\\n%.6f\" % score_list[2], \"\\n%.6f\" % score_list[3])\n",
    "\n",
    "    def metrics(self, true, pred):\n",
    "        tn, fp, fn, tp = cm(true, pred).ravel()\n",
    "        acc = (tp + tn)/(tn + fp + fn + tp)\n",
    "        prec = tp/(tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "        f1 = 2*(rec * prec) / (rec + prec)\n",
    "        return [acc, prec, rec, f1]\n",
    "\n",
    "    def evaluation(self):\n",
    "        svc_model = self.intitalize_model()\n",
    "\n",
    "        print(\"Evaluating the model on training dataset..\")\n",
    "        Y_train_pred = svc_model.predict(self.X_train)\n",
    "        train_score = self.metrics(self.Y_train, Y_train_pred)\n",
    "\n",
    "        print(\"Evaluating the model on testing dataset...\")\n",
    "        Y_test_pred = svc_model.predict(self.X_test)\n",
    "        test_score = self.metrics(self.Y_test, Y_test_pred)\n",
    "\n",
    "        print(\"Training Score\")\n",
    "        self.print_seq(train_score)\n",
    "\n",
    "        print(\"Testing Score\")\n",
    "        self.print_seq(test_score)\n",
    "        \n",
    "        return test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643281b",
   "metadata": {},
   "source": [
    "SVM class\n",
    "Contructor takes dataset and number of epochs as argument\n",
    "This class is used for training Linear SVM for different model type and mode of classification\n",
    "\n",
    "Methods:\n",
    "metrics:\n",
    "    args: true, pred {true means true label and pred means predicted label by perceptron}\n",
    "    Objective: To calculate accuracy, precision, recall and F-score with the help of confusion matrix\n",
    "    Return: list of metrics\n",
    "    \n",
    "print_seq:\n",
    "    args: list of metrics\n",
    "    Objective: printing the score\n",
    "    Return: None\n",
    "\n",
    "initialize_model:\n",
    "    args: None\n",
    "    Objective: intitalizing SVM model and fitting the model on training dataset\n",
    "    Return: svm model\n",
    "\n",
    "Evaluation:\n",
    "    args: None\n",
    "    Objective: predicting the svm model on testing dataset and calulating the performance of the trained svm model\n",
    "    Return: Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee68b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)  \n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457f71d",
   "metadata": {},
   "source": [
    "This FNN class is for simple mean model\n",
    "Constructor takes classification, vocab_size as argument\n",
    "The objective of this class is to generate a Feed Forward Neural Network with 50 neurons as first hidden layers ans 10 neurons as second hidden layer (given in the question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04e147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_vec(nn.Module):\n",
    "    def __init__(self, classification = \"binary\", vocab_size = 300):\n",
    "        super(MLP_vec, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        if classification == \"binary\":\n",
    "            self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        else:\n",
    "            # For multi-classification\n",
    "            self.fc3 = nn.Linear(hidden_2, 4)\n",
    "        self.prod = 10\n",
    "        self.fc1 = nn.Linear(vocab_size * self.prod, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6da676",
   "metadata": {},
   "source": [
    "This FNN class is for vector model like FNN part(b) where we need 10 x 300 feature vectors \n",
    "Constructor takes classification, vocab_size as argument\n",
    "Similar Objective as the simple mean FNN class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2744b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "class testData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.Y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369cc37",
   "metadata": {},
   "source": [
    "DataLoader for Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83063ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Data(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, Y_data):\n",
    "        \n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pad = np.zeros((50, 300), dtype = float)\n",
    "        pad[-len(self.X_data[index]):] = np.array(self.X_data[index])[:50]\n",
    "        X = torch.FloatTensor(pad)\n",
    "        Y = torch.tensor(self.Y_data[index])\n",
    "        #print(Y, \" <=> \", self.Y_data[index], ' <=> ', index)\n",
    "            \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cfbdb",
   "metadata": {},
   "source": [
    "DataLoader for RNN and GRU models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a4aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, model_type = \"rnn\"):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.model_type = model_type\n",
    "\n",
    "        #Defining the layers\n",
    "        if self.model_type == \"gru\":\n",
    "            self.layer = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        else:\n",
    "            self.layer = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(2500, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        out, hidden = self.layer(x, hidden)\n",
    "\n",
    "        out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb1f23",
   "metadata": {},
   "source": [
    "Model class for generating models for RNN and GRU based on which model_type we want to train\n",
    "Arguments: input_size: 300 vectors\n",
    "           output_size: depends upon classification type (binary classification or multiclass)\n",
    "           hidden_dim: 50 hidden dimensions\n",
    "           n_layers: number of RNN or GRU layers you want to add to the network\n",
    "           model_type: States which model (RNN or GRU) we want to intitalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d9204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datafile...\n",
      "Preproces: True\n",
      "Data Preprocessing Initiated...\n",
      "Train-Test Split Completed...\n",
      "Dataset Iterator formation...\n"
     ]
    }
   ],
   "source": [
    "\"\"\" READING THE DATA FILES\"\"\"\n",
    "print(\"Reading datafile...\")\n",
    "\n",
    "filename = \"./amazon_reviews_us_Kitchen_v1_00.tsv\"\n",
    "dt = DataTranformation(filename, True)\n",
    "\n",
    "print(\"Data Preprocessing Initiated...\")\n",
    "train, test = dt.train_test_split()\n",
    "print(\"Train-Test Split Completed...\")\n",
    "\n",
    "print(\"Dataset Iterator formation...\")\n",
    "sentences = Sentence(train['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "032d7968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Pretrained Model...\n",
      "[('queen', 0.7118193507194519)]\n",
      "0.5567486\n"
     ]
    }
   ],
   "source": [
    "# Pretrained Model\n",
    "print(\"Generating Pretrained Model...\")\n",
    "pretrained_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Calculating Pretrained Model's perforamance on given examples\n",
    "print(pretrained_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n",
    "print(pretrained_model.similarity('excellent', 'outstanding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f6b2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Model Generation...\n",
      "[('arthur', 0.47155138850212097)]\n",
      "0.79131085\n"
     ]
    }
   ],
   "source": [
    "# General Model\n",
    "print(\"General Model Generation...\")\n",
    "model = gensim.models.Word2Vec(sentences, vector_size = 300, min_count = 10, window = 11, seed = 200)\n",
    "\n",
    "# Calculating General Model's performance on given examples\n",
    "print(model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))\n",
    "print(model.wv.similarity('excellent', 'outstanding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8537e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: binary\n",
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: binary\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR PERCEPTRON, SVM AND FNN '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an instance of vectorizer to extract features\n",
    "\"\"\" MEAN FEATURES EXTRACTION \"\"\"\n",
    "vec_train = Vectorization(model = model, dataset = train)\n",
    "vec_test = Vectorization(model, test)\n",
    "\n",
    "X_train_model, Y_train_model = vec_train.feature_extraction()\n",
    "X_test_model, Y_test_model = vec_test.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR PERCEPTRON, SVM AND FNN \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98f67c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: multi-class\n",
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: multi-class\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR FNN '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" MEAN MULTI-CLASS FEATURES EXTRACTION \"\"\"\n",
    "vec_multi_train = Vectorization(model, train, classification = \"multi-class\")\n",
    "vec_multi_test = Vectorization(model, test, classification = \"multi-class\")\n",
    "\n",
    "X_train_multi, Y_train_multi = vec_multi_train.feature_extraction()\n",
    "X_test_multi, Y_test_multi = vec_multi_test.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR FNN \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84c7b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: binary\n",
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: binary\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR FNN '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TEN FEATURES IN A SINGLE ROW FEATURE EXTRACTION \"\"\"\n",
    "vec_mode_train = Vectorization(model, train, classification=\"binary\", mode=\"vec\")\n",
    "vec_mode_test = Vectorization(model, test, classification=\"binary\", mode=\"vec\")\n",
    "\n",
    "X_train_mode, Y_train_mode = vec_mode_train.feature_extraction()\n",
    "X_test_mode, Y_test_mode = vec_mode_test.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR FNN \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9f1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: multi-class\n",
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: multi-class\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR FNN '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TEN FEATURES IN A SINGLE ROW MULTI-CLASS FEATURES EXTRACTION \"\"\"\n",
    "vec_mode_train_multi = Vectorization(model, train, classification=\"multi-class\", mode=\"vec\")\n",
    "vec_mode_test_multi = Vectorization(model, test, classification=\"multi-class\", mode=\"vec\")\n",
    "\n",
    "X_train_mode_multi, Y_train_mode_multi = vec_mode_train_multi.feature_extraction()\n",
    "X_test_mode_multi, Y_test_mode_multi = vec_mode_test_multi.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR FNN \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d73d7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: binary\n",
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: binary\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR PERCEPTRON AND SVM '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"PRETRAINED MODEL FEATURES EXTRACTION\"\"\"\n",
    "vec2_train = Vectorization(model = pretrained_model, dataset = train, model_type = \"pretrained\")\n",
    "vec2_test = Vectorization(model = pretrained_model, dataset = test, model_type = \"pretrained\")\n",
    "\n",
    "X_train_pre, Y_train_pre = vec2_train.feature_extraction()\n",
    "X_test_pre, Y_test_pre = vec2_test.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR PERCEPTRON AND SVM \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "375972cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: multi-class\n",
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: multi-class\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR FNN '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" PRETRAINED MODEL MULTI-CLASS FEATURES EXTRACTION \"\"\"\n",
    "\n",
    "vec2_multi_train = Vectorization(model = pretrained_model, dataset = train, classification = \"multi-class\", model_type=\"pretrained\")\n",
    "vec2_multi_test = Vectorization(model = pretrained_model, dataset = test, classification = \"multi-class\", model_type = \"pretrained\")\n",
    "\n",
    "X_train_multi_pre, Y_train_multi_pre = vec2_multi_train.feature_extraction()\n",
    "X_test_multi_pre, Y_test_multi_pre = vec2_multi_test.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR FNN \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e2b7ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: binary\n",
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: binary\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR FNN '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" PRETRAINED MODE VEC BINARY FEATURES EXTRACTION \"\"\"\n",
    "\n",
    "vec_mode_train_pre = Vectorization(model = pretrained_model, dataset = train, model_type=\"pretrained\", mode=\"vec\")\n",
    "vec_mode_test_pre = Vectorization(model = pretrained_model, dataset = test, model_type=\"pretrained\", mode = \"vec\")\n",
    "\n",
    "X_train_mode_pre, Y_train_mode_pre = vec_mode_train_pre.feature_extraction()\n",
    "X_test_mode_pre, Y_test_mode_pre = vec_mode_test_pre.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR FNN \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f961c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: multi-class\n",
      "Vectorizing training dataset....\n",
      "Model Type: pretrained\n",
      "Classification: multi-class\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR FNN '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" PRETRAINED MDOE VEC MULTI-CLASS FEATURES EXTRACTION \"\"\"\n",
    "\n",
    "vec_mode_train_multi_pre = Vectorization(model = pretrained_model, dataset = train, classification = \"multi-class\", model_type=\"pretrained\", mode = \"vec\")\n",
    "vec_mode_test_multi_pre = Vectorization(model = pretrained_model, dataset = test, classification = \"multi-class\", model_type = \"pretrained\", mode = \"vec\")\n",
    "\n",
    "X_train_mode_multi_pre, Y_train_mode_multi_pre = vec_mode_train_multi_pre.feature_extraction()\n",
    "X_test_mode_multi_pre, Y_test_mode_multi_pre = vec_mode_test_multi_pre.feature_extraction()\n",
    "\n",
    "\"\"\" TO BE USED FOR FNN \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed3c4a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TO BE USED FOR PERCEPTRON AND SVM '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\" TFIDF FEATURES EXTRACTION \"\"\"\n",
    "    # TFIDF\n",
    "def get_tfidf(train, test):\n",
    "    train_x = train.apply(lambda x: \" \".join(ele for ele in x))\n",
    "    test_x = test.apply(lambda x: \" \".join(ele for ele in x))\n",
    "    tfidf_vect = TfidfVectorizer(min_df = 0.001)\n",
    "    train_x_vectors = tfidf_vect.fit_transform(train_x)\n",
    "    train_x_vectors = pd.DataFrame(train_x_vectors.toarray(), columns = tfidf_vect.get_feature_names())\n",
    "    test_x_vectors = tfidf_vect.transform(test_x)\n",
    "    test_x_vectors = pd.DataFrame(test_x_vectors.toarray(), columns = tfidf_vect.get_feature_names())\n",
    "    return train_x_vectors, test_x_vectors\n",
    "        \n",
    "train_tfidf = train[train.label != 3].reset_index(drop = True)\n",
    "test_tfidf = test[test.label != 3].reset_index(drop = True)\n",
    "X_train_tfidf, X_test_tfidf = get_tfidf(train_tfidf.review_body, test_tfidf.review_body)\n",
    "Y_train_tfidf = train_tfidf['label']\n",
    "Y_test_tfidf = test_tfidf['label']\n",
    "\n",
    "\"\"\" TO BE USED FOR PERCEPTRON AND SVM \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b53cf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the Model...\n",
      "-- Epoch 1\n",
      "Norm: 1.29, NNZs: 300, Bias: 0.000000, T: 159848, Avg. loss: 0.036654\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.36, NNZs: 300, Bias: -0.050000, T: 319696, Avg. loss: 0.036680\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.44, NNZs: 300, Bias: 0.010000, T: 479544, Avg. loss: 0.036638\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.51, NNZs: 300, Bias: -0.070000, T: 639392, Avg. loss: 0.036496\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.54, NNZs: 300, Bias: -0.030000, T: 799240, Avg. loss: 0.036516\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.52, NNZs: 300, Bias: -0.020000, T: 959088, Avg. loss: 0.036951\n",
      "Total training time: 0.58 seconds.\n",
      "Convergence after 6 epochs took 0.59 seconds\n",
      "Evaluating the model on training dataset..\n",
      "Evaluating the model on testing dataset...\n",
      "Training Score\n",
      "0.819854 \n",
      "0.845115 \n",
      "0.782099 \n",
      "0.812387\n",
      "Testing Score\n",
      "0.816328 \n",
      "0.846522 \n",
      "0.778234 \n",
      "0.810943\n",
      "Fitting the Model...\n",
      "-- Epoch 1\n",
      "Norm: 0.38, NNZs: 300, Bias: -0.010000, T: 159831, Avg. loss: 0.002503\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.38, NNZs: 300, Bias: -0.020000, T: 319662, Avg. loss: 0.002502\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.41, NNZs: 300, Bias: -0.020000, T: 479493, Avg. loss: 0.002507\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.41, NNZs: 300, Bias: -0.020000, T: 639324, Avg. loss: 0.002511\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.39, NNZs: 300, Bias: -0.020000, T: 799155, Avg. loss: 0.002505\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.40, NNZs: 300, Bias: -0.020000, T: 958986, Avg. loss: 0.002495\n",
      "Total training time: 0.49 seconds.\n",
      "Convergence after 6 epochs took 0.49 seconds\n",
      "Evaluating the model on training dataset..\n",
      "Evaluating the model on testing dataset...\n",
      "Training Score\n",
      "0.763225 \n",
      "0.690646 \n",
      "0.951428 \n",
      "0.800329\n",
      "Testing Score\n",
      "0.767361 \n",
      "0.698484 \n",
      "0.950958 \n",
      "0.805398\n",
      "Fitting the Model...\n",
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 3104, Bias: 0.000000, T: 160035, Avg. loss: 0.001609\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.83, NNZs: 3104, Bias: -0.010000, T: 320070, Avg. loss: 0.001623\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.85, NNZs: 3104, Bias: 0.000000, T: 480105, Avg. loss: 0.001630\n",
      "Total training time: 1.43 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.85, NNZs: 3104, Bias: 0.000000, T: 640140, Avg. loss: 0.001640\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.86, NNZs: 3104, Bias: -0.010000, T: 800175, Avg. loss: 0.001629\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.87, NNZs: 3104, Bias: -0.010000, T: 960210, Avg. loss: 0.001616\n",
      "Total training time: 2.87 seconds.\n",
      "Convergence after 6 epochs took 2.87 seconds\n",
      "Evaluating the model on training dataset..\n",
      "Evaluating the model on testing dataset...\n",
      "Training Score\n",
      "0.783854 \n",
      "0.956979 \n",
      "0.593071 \n",
      "0.732307\n",
      "Testing Score\n",
      "0.778406 \n",
      "0.954277 \n",
      "0.590347 \n",
      "0.729439\n"
     ]
    }
   ],
   "source": [
    "# PERCEPTRON\n",
    "\n",
    "per = Percept(X_train = X_train_model, Y_train = Y_train_model, X_test = X_test_model, Y_test = Y_test_model)\n",
    "model_test_score = per.evaluation()\n",
    "\n",
    "per2 = Percept(X_train = X_train_pre, Y_train = Y_train_pre, X_test = X_test_pre, Y_test = Y_test_pre)\n",
    "model_pre_test_score = per2.evaluation()\n",
    "\n",
    "per3 = Percept(X_train = X_train_tfidf, Y_train = Y_train_tfidf, X_test = X_test_tfidf, Y_test = Y_test_tfidf)\n",
    "model_tfidf_test_score = per3.evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a55219bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the SVM\n",
      "Evaluating the model on training dataset..\n",
      "Evaluating the model on testing dataset...\n",
      "Training Score\n",
      "0.867518 \n",
      "0.860909 \n",
      "0.875845 \n",
      "0.868313\n",
      "Testing Score\n",
      "0.868509 \n",
      "0.866467 \n",
      "0.875087 \n",
      "0.870756\n",
      "Fitting the SVM\n",
      "Evaluating the model on training dataset..\n",
      "Evaluating the model on testing dataset...\n",
      "Training Score\n",
      "0.829827 \n",
      "0.812739 \n",
      "0.856039 \n",
      "0.833827\n",
      "Testing Score\n",
      "0.830619 \n",
      "0.818565 \n",
      "0.854902 \n",
      "0.836339\n",
      "Fitting the SVM\n",
      "Evaluating the model on training dataset..\n",
      "Evaluating the model on testing dataset...\n",
      "Training Score\n",
      "0.892099 \n",
      "0.888155 \n",
      "0.896438 \n",
      "0.892277\n",
      "Testing Score\n",
      "0.886275 \n",
      "0.885772 \n",
      "0.890021 \n",
      "0.887891\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "svm = SVM(X_train = X_train_model, Y_train = Y_train_model, X_test = X_test_model, Y_test = Y_test_model)\n",
    "svm_model_test_score = svm.evaluation()\n",
    "\n",
    "svm2 = SVM(X_train = X_train_pre, Y_train = Y_train_pre, X_test = X_test_pre, Y_test = Y_test_pre)\n",
    "svm_pre_test_score = svm2.evaluation()\n",
    "\n",
    "svm3 = SVM(X_train = X_train_tfidf, Y_train = Y_train_tfidf, X_test = X_test_tfidf, Y_test = Y_test_tfidf)\n",
    "svm_tfidf_test_score = svm3.evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb9591c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CLEANING TO FREE SOME MEMORY \"\"\"\n",
    "    \n",
    "del per3, svm3, model_tfidf_test_score, svm_tfidf_test_score,\\\n",
    "    train_tfidf, test_tfidf, X_train_tfidf, Y_train_tfidf, X_test_tfidf,\\\n",
    "    Y_test_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e6260",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    Perceptron: Trained Word2Vec Model's Accuracy -> 0.816\n",
    "                Pretrained Word2Vec Model's Accuracy -> 0.767\n",
    "                TFIDF Model's Accuracy -> 0.778\n",
    "    Conclusion: Trained Word2Vec Model performed better compare to pretrained and TFIDF models\n",
    "    \n",
    "    SVM: Trained Word2Vec Model's Accuracy -> 0.868\n",
    "         Pretrained Word2Vec Model's Accuracy ->0.830\n",
    "         TFIDF Model's Accuracy -> 0.886\n",
    "    Conclusion: TFIDF model performed better compare to other models. Our trained model's F-score is just 0.01 below compare to \n",
    "                TFIDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "373d3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TRAINING FUNCTION \"\"\"\n",
    "    \n",
    "def training(model, epoch, dataset_x, dataset_y, name = \"model\"):\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    print(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.01)\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    training_data = trainData(torch.FloatTensor(dataset_x), torch.LongTensor(dataset_y))\n",
    "    train_loader = DataLoader(dataset = training_data, batch_size=16, shuffle = True)\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        mlp_model.train()\n",
    "        for input_data, label in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = mlp_model(input_data.to(device))\n",
    "            loss = criterion(output, label.to(device)) #y_batch.unsqueeze(1) (label.unsqueeze(1)).to(device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * input_data.size(1)\n",
    "\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "        #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        torch.save(mlp_model.state_dict(), name + str(epoch + 1) + '.pt')\n",
    "        \n",
    "\"\"\" TESTING FUNCTION \"\"\"\n",
    "    \n",
    "def testing(model, epoch, dataset_x, dataset_y, name = \"model\"):\n",
    "    max_acc = 0\n",
    "    device = torch.device('cpu')\n",
    "    testing_data = testData(torch.FloatTensor(dataset_x), torch.LongTensor(dataset_y))\n",
    "    test_loader = DataLoader(dataset = testing_data, batch_size=16)\n",
    "\n",
    "    for i in range(1, epoch + 1):\n",
    "\n",
    "        model.load_state_dict(torch.load(name +str(i) + '.pt'))\n",
    "        model = model.to(device)\n",
    "\n",
    "        predictions, actual = list(), list()\n",
    "        for test_data, test_label in test_loader:\n",
    "\n",
    "            pred = mlp_model(test_data.to(device))\n",
    "            pred = pred.detach().numpy()\n",
    "            pred = argmax(pred, axis= 1)\n",
    "            target = test_label.numpy()\n",
    "            target = target.reshape((len(target), 1))\n",
    "            pred = pred.reshape((len(pred)), 1)\n",
    "            pred = pred.round()\n",
    "            predictions.append(pred)\n",
    "            actual.append(target)\n",
    "\n",
    "        predictions, actual = vstack(predictions), vstack(actual)\n",
    "        acc = accuracy_score(actual, predictions)\n",
    "        max_acc = max(max_acc, acc)\n",
    "    print('Accuracy: %.3f' % max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e7e7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b483f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.884\n"
     ]
    }
   ],
   "source": [
    "\"\"\" BINARY-MEAN MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP() # binary classification\n",
    "training(mlp_model, 10, X_train_model, Y_train_model, name = \"mlp_model\")\n",
    "testing(mlp_model, 10, X_test_model, Y_test_model, name = \"mlp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f73f7a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.721\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MULTI-CLASS MEAN MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP(classification = \"multi-class\")\n",
    "training(mlp_model, 10, X_train_multi, Y_train_multi, name = \"mlp_model_multi\")\n",
    "testing(mlp_model, 10, X_test_multi, Y_test_multi, name = \"mlp_model_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f83281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_vec(\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.770\n"
     ]
    }
   ],
   "source": [
    "\"\"\" BINARY-VEC MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec()\n",
    "training(mlp_model, 10, X_train_mode, Y_train_mode, name = \"mlp_mode_vec\")\n",
    "testing(mlp_model, 10, X_test_mode, Y_test_mode, name = \"mlp_mode_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2d8e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_vec(\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.617\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MULTI-VEC MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec(classification=\"multi-class\")\n",
    "training(mlp_model, 10, X_train_mode_multi, Y_train_mode_multi, name = \"mlp_mode_vec_multi\")\n",
    "testing(mlp_model, 10, X_test_mode_multi, Y_test_mode_multi, name = \"mlp_mode_vec_multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "261ca259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "\"\"\" BINARY-MEAN PRETRAINED MLP \"\"\"\n",
    "mlp_model = MLP()\n",
    "training(mlp_model, 10, X_train_pre, Y_train_pre, name = \"mlp_model_pre\")\n",
    "testing(mlp_model, 10, X_test_pre, Y_test_pre, name = \"mlp_model_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e677147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.679\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MULTI-CLASS MEAN PRETRAINED MLP \"\"\"\n",
    "mlp_model = MLP(classification = \"multi-class\")\n",
    "training(mlp_model, 10, X_train_multi_pre, Y_train_multi_pre, name = \"mlp_mode_multi_pre\")\n",
    "testing(mlp_model, 10, X_test_multi_pre, Y_test_multi_pre, name = \"mlp_mode_multi_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d510240e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_vec(\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.755\n"
     ]
    }
   ],
   "source": [
    "\"\"\" BINARY VEC PRETRAINED MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec()\n",
    "training(mlp_model, 10, X_train_mode_pre, Y_train_mode_pre, name = \"mlp_vec_pre\")\n",
    "testing(mlp_model, 10, X_test_mode_pre, Y_test_mode_pre, name = \"mlp_vec_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c00f3771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_vec(\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (sig): Sigmoid()\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n",
      "Accuracy: 0.608\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MULTI-CLASS VEC PRETRAINED MLP \"\"\"\n",
    "\n",
    "mlp_model = MLP_vec(classification = \"multi-class\")\n",
    "training(mlp_model, 10, X_train_mode_multi_pre, Y_train_mode_multi_pre, name = \"mlp_vec_multi_pre\")\n",
    "testing(mlp_model, 10, X_test_mode_multi_pre, Y_test_mode_multi_pre, name = \"mlp_vec_multi_pre\")\n",
    "\n",
    "del mlp_model, X_train_mode_multi_pre, Y_train_mode_multi_pre, X_test_mode_multi_pre, Y_test_mode_multi_pre,\\\n",
    "    X_train_mode_pre, Y_train_mode_pre, X_test_mode_pre, Y_test_mode_pre, X_train_multi_pre, Y_train_multi_pre,\\\n",
    "    X_test_multi_pre, Y_test_multi_pre, X_train_pre, Y_train_pre, X_test_pre, Y_test_pre, X_train_mode_multi,\\\n",
    "    Y_train_mode_multi, X_test_mode_multi, Y_test_mode_multi, X_train_mode, Y_train_mode, X_test_mode, Y_test_mode,\\\n",
    "    X_train_multi, Y_train_multi, X_test_multi, Y_test_multi, X_train_model, Y_train_model, X_test_model, Y_test_model\n",
    "\n",
    "del vec_mode_train_multi_pre, vec_mode_test_multi_pre, vec_mode_train_pre, vec_mode_test_pre, vec2_multi_train,\\\n",
    "    vec2_multi_test, vec2_train, vec2_test, vec_mode_train_multi, vec_mode_test_multi, vec_mode_train, vec_mode_test,\\\n",
    "    vec_multi_train, vec_multi_test, vec_train, vec_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda79bd",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    BINARY-MEAN FNN -> 0.884\n",
    "    MULTI-CLASS MEAN FNN -> 0.721\n",
    "    BINARY-VEC FNN -> 0.770\n",
    "    MULTI-VEC FNN -> 0.617\n",
    "    \n",
    "    BINARY-MEAN PRETRAINED FNN -> 0.838\n",
    "    MULTI-CLASS MEAN PRETRAINED FNN -> 0.679\n",
    "    BINARY VEC PRETRAINED FNN -> 0.755\n",
    "    MULTI-CLASS VEC PRETRAINED FNN -> 0.608\n",
    "    \n",
    "Conclusion: As we can see here In all the cases our trained model ouperforms the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6be2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [item[1] for item in batch]\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def rnn_train(model, epoch, dataset_x, dataset_y, name):\n",
    "\n",
    "    rnn_train = RNN_Data(dataset_x, dataset_y)\n",
    "    train_loader_mode = DataLoader(dataset = rnn_train, batch_size=8, shuffle = True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for ep in range(1, epoch + 1):\n",
    "\n",
    "        for input_data, label in train_loader_mode:\n",
    "            optimizer.zero_grad()\n",
    "            input_data = torch.stack(input_data)\n",
    "            label = torch.stack(label)\n",
    "            output, hidden = model(input_data.to(device))\n",
    "            loss = criterion(output, label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #print('Epoch: {} \\tTraining Loss: {:.6f}'.format(ep, loss.item()))\n",
    "        torch.save(model.state_dict(), name + str(ep) + '.pt')\n",
    "\n",
    "def rnn_test(model, epoch, dataset_x, dataset_y, name):\n",
    "    max_acc = 0\n",
    "    rnn_test = RNN_Data(dataset_x, dataset_y)\n",
    "    test_loader_mode = DataLoader(dataset = rnn_test, batch_size=8, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "    for i in range(1, epoch + 1):\n",
    "\n",
    "        model.load_state_dict(torch.load(name +str(i) + '.pt'))\n",
    "        model = model.to(device)\n",
    "\n",
    "        predictions, actual = list(), list()\n",
    "        for test_data, test_label in test_loader_mode:\n",
    "            test_data = torch.stack(test_data)\n",
    "            test_label = torch.stack(test_label)\n",
    "            pred, hid = model(test_data.to(device))\n",
    "            pred = pred.to('cpu')\n",
    "            pred = pred.detach().numpy()\n",
    "            pred = argmax(pred, axis= 1)\n",
    "            target = test_label.numpy()\n",
    "            target = target.reshape((len(target), 1))\n",
    "            pred = pred.reshape((len(pred)), 1)\n",
    "            pred = pred.round()\n",
    "            predictions.append(pred)\n",
    "            actual.append(target)\n",
    "\n",
    "        predictions, actual = vstack(predictions), vstack(actual)\n",
    "        acc = accuracy_score(actual, predictions)\n",
    "        max_acc = max(max_acc, acc)\n",
    "    print('Accuracy: %.3f' % max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33def06f",
   "metadata": {},
   "source": [
    "Collate Function: a custom batch loader for DataLoader. It is used if the size of the dataset is volatile\n",
    "rnn_train and rnn_test are training and testing functions. Both functions take model, dataset, number of epochs and name as arguments.\n",
    "These both functions are being used to train and test both RNN and GRU models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f36e2962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.766\n",
      "Accuracy: 0.770\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    RNN DATASET PREPARATION\n",
    "    SAME FOR GRU...    \n",
    "\"\"\"\n",
    "rnn_bin = Model(300, 3, 50, 1)\n",
    "rnn_bin = rnn_bin.to(device)\n",
    "gru_model_bin = Model(300, 3, 50, 1, model_type=\"gru\")\n",
    "gru_model_bin = gru_model_bin.to(device)\n",
    "\n",
    "vec_rnn_train = Vectorization(model, train, classification = \"binary\", pad = True)\n",
    "vec_rnn_test = Vectorization(model, test, classification =\"binary\", pad = True)\n",
    "\n",
    "X_rnn_train, Y_rnn_train = vec_rnn_train.feature_extraction()\n",
    "X_rnn_test, Y_rnn_test = vec_rnn_test.feature_extraction()\n",
    "\n",
    "\n",
    "rnn_train(rnn_bin, 10, X_rnn_train, Y_rnn_train, name = \"rnn_model\")\n",
    "rnn_test(rnn_bin, 10, X_rnn_test, Y_rnn_test, name = \"rnn_model\")\n",
    "\n",
    "rnn_train(gru_model_bin, 10, X_rnn_train, Y_rnn_train, name = \"gru_model\")\n",
    "rnn_test(gru_model_bin, 10, X_rnn_test, Y_rnn_test, name = \"gru_model\")\n",
    "\n",
    "del vec_rnn_train, vec_rnn_test, X_rnn_train, X_rnn_test, Y_rnn_train, Y_rnn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "307db833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: multi-class\n",
      "Vectorizing training dataset....\n",
      "Model Type: model\n",
      "Classification: multi-class\n",
      "Vectorization Completed\n",
      "Vectorization Completed\n",
      "Accuracy: 0.605\n",
      "Accuracy: 0.628\n"
     ]
    }
   ],
   "source": [
    "rnn = Model(300, 4, 50, 1)\n",
    "rnn = rnn.to(device)\n",
    "vec_rnn_multi_train = Vectorization(model, train, classification = \"multi-class\", pad = True)\n",
    "vec_rnn_multi_test = Vectorization(model, test, classification = \"multi-class\", pad = True)\n",
    "\n",
    "X_rnn_multi_train, Y_rnn_multi_train = vec_rnn_multi_train.feature_extraction()\n",
    "X_rnn_multi_test, Y_rnn_multi_test = vec_rnn_multi_test.feature_extraction()\n",
    "\n",
    "rnn_train(rnn, 10, X_rnn_multi_train, Y_rnn_multi_train, name = \"rnn_multi_model\")\n",
    "rnn_test(rnn, 10, X_rnn_multi_test, Y_rnn_multi_test, name = \"rnn_multi_model\")\n",
    "\n",
    "gru_model = Model(300, 4, 50, 1, model_type=\"gru\")\n",
    "gru_model = gru_model.to(device)\n",
    "\n",
    "rnn_train(gru_model, 10, X_rnn_multi_train, Y_rnn_multi_train, name = \"gru_multi_model\")\n",
    "rnn_test(gru_model, 10, X_rnn_multi_test, Y_rnn_multi_test, name = \"gru_multi_model\")\n",
    "\n",
    "del vec_rnn_multi_train, vec_rnn_multi_test, Y_rnn_multi_train, X_rnn_multi_test, Y_rnn_multi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0511e2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.820\n",
      "Accuracy: 0.871\n"
     ]
    }
   ],
   "source": [
    "vec_rnn_pre_train = Vectorization(model = pretrained_model, dataset = train, model_type=\"pretrained\", classification = \"binary\", mode = \"vec\", pad = True)\n",
    "vec_rnn_pre_test = Vectorization(model = pretrained_model, dataset = test, model_type = \"pretrained\", classification = \"binary\", mode = \"vec\", pad = True)\n",
    "\n",
    "X_rnn_pre_train, Y_rnn_pre_train = vec_rnn_pre_train.feature_extraction()\n",
    "X_rnn_pre_test, Y_rnn_pre_test = vec_rnn_pre_test.feature_extraction()\n",
    "\n",
    "rnn_bin = Model(300, 3, 50, 1)\n",
    "rnn_bin = rnn_bin.to(device)\n",
    "gru_model_bin = Model(300, 3, 50, 1, model_type=\"gru\")\n",
    "gru_model_bin = gru_model_bin.to(device)\n",
    "\n",
    "rnn_train(rnn_bin, 10, X_rnn_pre_train, Y_rnn_pre_train, name = \"rnn_pre_model_bin\")\n",
    "rnn_test(rnn_bin, 10, X_rnn_pre_test, Y_rnn_pre_test, name = \"rnn_pre_model_bin\")\n",
    "\n",
    "rnn_train(gru_model_bin, 10, X_rnn_pre_train, Y_rnn_pre_train, name = \"gru_pre_model_bin\")\n",
    "rnn_test(gru_model_bin, 10, X_rnn_pre_test, Y_rnn_pre_test, name = \"gru_pre_model_bin\")\n",
    "\n",
    "del vec_rnn_pre_train, vec_rnn_pre_test,  X_rnn_pre_train, Y_rnn_pre_train, X_rnn_pre_test, Y_rnn_pre_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfaf4343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.672\n",
      "Accuracy: 0.715\n"
     ]
    }
   ],
   "source": [
    "vec_rnn_pre_multi_train = Vectorization(model = pretrained_model, dataset = train, model_type = \"pretrained\", classification = \"multi-class\", mode = \"vec\", pad = True)\n",
    "vec_rnn_pre_multi_test = Vectorization(model = pretrained_model, dataset = test, model_type = \"pretrained\", classification = \"multi-class\", mode = \"vec\", pad = True)\n",
    "\n",
    "X_rnn_pre_multi_train, Y_rnn_pre_multi_train = vec_rnn_pre_multi_train.feature_extraction()\n",
    "X_rnn_pre_multi_test, Y_rnn_pre_multi_test = vec_rnn_pre_multi_test.feature_extraction()\n",
    "\n",
    "rnn = Model(300, 4, 50, 1)\n",
    "rnn = rnn.to(device)\n",
    "gru_model = Model(300, 4, 50, 1, model_type=\"gru\")\n",
    "gru_model = gru_model.to(device)\n",
    "\n",
    "rnn_train(rnn, 10, X_rnn_pre_multi_train, Y_rnn_pre_multi_train, name = \"rnn_pre_model\")\n",
    "rnn_test(rnn, 10, X_rnn_pre_multi_test, Y_rnn_pre_multi_test, name = \"rnn_pre_model\")\n",
    "\n",
    "rnn_train(gru_model, 10, X_rnn_pre_multi_train, Y_rnn_pre_multi_train, name = \"gru_pre_model\")\n",
    "rnn_test(gru_model, 10, X_rnn_pre_multi_test, Y_rnn_pre_multi_test, name = \"gru_pre_model\")\n",
    "\n",
    "del vec_rnn_pre_multi_train, vec_rnn_pre_multi_test,  X_rnn_pre_multi_train, Y_rnn_pre_multi_train, X_rnn_pre_multi_test, Y_rnn_pre_multi_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea076e2",
   "metadata": {},
   "source": [
    "Observation:\n",
    "    RNN:\n",
    "        Binary model -> 0.766 (Our trained model)\n",
    "        Multi-class -> 0.605 (Our trained model)\n",
    "        Binary model -> 0.820 (pretrained model)\n",
    "        Multi-class -> 0.871 (pretrained model)\n",
    "    \n",
    "    GRU:\n",
    "        Binary model -> 0.770 (Our Trained Model)\n",
    "        Multi-class -> 0.628 (Our Trained Model)\n",
    "        Binary model -> 0.672 (Pretrained Model)\n",
    "        Multi-class -> 0.715 (Pretrained Model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
