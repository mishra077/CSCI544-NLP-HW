{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/train\", sep = \"\\t\", names = ['id', 'words', 'pos'])\n",
    "df['occ'] = df.groupby('words')[\"words\"].transform('size')\n",
    "def replace(row):\n",
    "    if row.occ <= 1:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return row.words\n",
    "df['words'] = df.apply(lambda row : replace(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected threshold for unknown words:  1\n",
      "Total size of the vocabulary:  23183\n",
      "Total occurrences of the special token <unk>:  20011\n"
     ]
    }
   ],
   "source": [
    "df_vocab = df.words.value_counts().rename_axis('words').reset_index(name = 'occ')\n",
    "df_unk = df_vocab[df_vocab['words'] == \"<unk>\"]\n",
    "index = df_vocab[df_vocab.words == \"<unk>\"].index\n",
    "df_vocab = df_vocab.drop(index)\n",
    "df_vocab = pd.concat([df_unk, df_vocab]).reset_index(drop = True)\n",
    "df_vocab['id'] = df_vocab.index + 1\n",
    "cols = df_vocab.columns.tolist()\n",
    "cols = [cols[0], cols[-1], cols[1]]\n",
    "df_vocab = df_vocab[cols]\n",
    "df_vocab.to_csv(\"vocab.txt\", sep=\"\\t\", header=None)\n",
    "print(\"Selected threshold for unknown words: \", 1)\n",
    "print(\"Total size of the vocabulary: \", df_vocab.shape[0])\n",
    "print(\"Total occurrences of the special token <unk>: \", int(df_vocab[df_vocab[\"words\"] == \"<unk>\"].occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "df_pos = df.pos.value_counts().rename_axis('pos').reset_index(name = 'count')\n",
    "pos_dict = dict(df_pos.values)\n",
    "tags = df_pos.pos.tolist() # Extracting all the distinc tags\n",
    "print(len(tags))\n",
    "\"\"\"\n",
    "    Creating the 2D list of sentences where in each entry we have 1D list of sentence and in each sentence we have tuples of word and\n",
    "    its corresponding pos tag.\n",
    "\"\"\"\n",
    "sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for row in df.itertuples():\n",
    "    if(row.id == 1 and first == 0):\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append((row.words, row.pos))\n",
    "sentences.append(sentence)\n",
    "del(df_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2 HMM LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    get_trans_matrix\n",
    "        args: sentences (2D list from training dataseet)\n",
    "              tags: list of distinct tags\n",
    "    \n",
    "        return: Transition matrix (2D numpy array square matrix)\n",
    "        size: length of tags * length of tags\n",
    "\n",
    "        row: tag at previous state\n",
    "        col: tag at current state we want to calulate for\n",
    "\n",
    "        formula: transition from s -> s' = count(s->s') / count(s)\n",
    "\"\"\"\n",
    "\n",
    "def get_trans_matrix(sentences, tags):\n",
    "    tr_matrix = np.zeros((len(tags),len(tags)))\n",
    "\n",
    "    tag_occ = {}\n",
    "    for tag in range(len(tags)):\n",
    "        tag_occ[tag] = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            tag_occ[tags.index(sentence[i][1])] += 1\n",
    "            if i == 0: continue\n",
    "            tr_matrix[tags.index(sentence[i - 1][1])][tags.index(sentence[i][1])] += 1\n",
    "    \n",
    "    for i in range(tr_matrix.shape[0]):\n",
    "        for j in range(tr_matrix.shape[1]):\n",
    "            if(tr_matrix[i][j] == 0) : tr_matrix[i][j] = 1e-10\n",
    "            else: tr_matrix[i][j] /= tag_occ[i]\n",
    "\n",
    "    return tr_matrix\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    get_emission_matrix\n",
    "        args: tags (list of distinct pos tags)\n",
    "              vocab (list of all distinct words in the training dataset)\n",
    "              sentences (2D list of all the sentence in the training dataset)\n",
    "\n",
    "        return: emission matrix (2D numpy array)\n",
    "        size: length of tags * length of vocabualry\n",
    "\n",
    "        row : tags\n",
    "        col: vocab\n",
    "\n",
    "        Formula: emission probability = from pos tag to a word = count(s -> x) / count(s)\n",
    "\"\"\"\n",
    "\n",
    "def get_emission_matrix(tags, vocab, sentences):\n",
    "    em_matrix = np.zeros((len(tags), len(vocab)))\n",
    "\n",
    "    tag_occ = {}\n",
    "    for tag in range(len(tags)):\n",
    "        tag_occ[tag] = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, pos in sentence:\n",
    "            tag_occ[tags.index(pos)] +=1\n",
    "            em_matrix[tags.index(pos)][vocab.index(word)] += 1\n",
    "\n",
    "    for i in range(em_matrix.shape[0]):\n",
    "        for j in range(em_matrix.shape[1]):\n",
    "            if(em_matrix[i][j] == 0) : em_matrix[i][j] = 1e-10\n",
    "            else: em_matrix[i][j] /= tag_occ[i]\n",
    "\n",
    "    return em_matrix\n",
    "\n",
    "# For any entry in emission matrix or transition matrix if the entry is zero we are inserting 1e-10 as the probability.\n",
    "\n",
    "vocab = df_vocab.words.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transition parameter in our HMM model: 2070\n",
      "Total emission paramenter in our HMM model: 1043235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Creating the dictionary for transition and emission matrix\n",
    "    Each cell in either cell states either transition from one\n",
    "    tag to other tag or going from a tag to a word. These states\n",
    "    will be the key for their respective dictionaries\n",
    "\"\"\"\n",
    "def get_trans_probs(tags, tr_matrix):\n",
    "    tags_dict = {}\n",
    "\n",
    "    for i, tags in enumerate(tags):\n",
    "        tags_dict[i] = tags\n",
    "\n",
    "    trans_prob = {}\n",
    "    for i in range(tr_matrix.shape[0]):\n",
    "        for j in range(tr_matrix.shape[1]):\n",
    "            trans_prob['(' + tags_dict[i] + ',' + tags_dict[j] + ')'] = tr_matrix[i][j]\n",
    "\n",
    "\n",
    "    return trans_prob\n",
    "\n",
    "def get_emission_probs(tags, vocab, em_matrix):\n",
    "    tags_dict = {}\n",
    "\n",
    "    for i, tags in enumerate(tags):\n",
    "        tags_dict[i] = tags\n",
    "\n",
    "    emission_probs = {}\n",
    "\n",
    "    for i in range(em_matrix.shape[0]):\n",
    "        for j in range(em_matrix.shape[1]):\n",
    "            emission_probs['(' + tags_dict[i] + ', ' + vocab[j] + ')'] = em_matrix[i][j]\n",
    "\n",
    "    return emission_probs\n",
    "\n",
    "\n",
    "def get_all_prob(tags, vocab, sentences):\n",
    "    tr_matrix = get_trans_matrix(sentences, tags)\n",
    "    em_matrix = get_emission_matrix(tags, vocab, sentences)\n",
    "                \n",
    "    transition_probability = get_trans_probs(tags, tr_matrix)\n",
    "    emission_probability = get_emission_probs(tags, vocab, em_matrix)\n",
    "\n",
    "    return transition_probability, emission_probability\n",
    "\n",
    "\"\"\"\n",
    "    Initial Probability\n",
    "        args: df (dataframe)\n",
    "              tags (list of pos)\n",
    "\n",
    "        return: a dictionary of intital probability\n",
    "\n",
    "        Objective: For calculating T(s1). \n",
    "\n",
    "        Formula: T(s1) = count(s1 at the begining) / sum of all count(s at the begining)\n",
    "\"\"\"\n",
    "\n",
    "def get_inital_prob(df, tags):\n",
    "    tags_start_occ = {}\n",
    "    total_start_sum = 0\n",
    "    for tag in tags:\n",
    "        tags_start_occ[tag] = 0\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        if(row[1] == 1):\n",
    "            tags_start_occ[row[3]]+=1\n",
    "            total_start_sum += 1\n",
    "    \n",
    "    prior_prob = {}\n",
    "    for key in tags_start_occ:\n",
    "        prior_prob[key] = tags_start_occ[key] / total_start_sum\n",
    "    \n",
    "    return prior_prob\n",
    "\n",
    "# Extract all the probaility dictionary\n",
    "trans_prob, em_prob = get_all_prob(tags, vocab, sentences)\n",
    "prior_prob = get_inital_prob(df, tags)\n",
    "print(\"Total transition parameter in our HMM model: {}\".format(len(trans_prob) + len(prior_prob)))\n",
    "print(\"Total emission paramenter in our HMM model: {}\".format(len(em_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trans_prob = {}\n",
    "for key in prior_prob:\n",
    "    total_trans_prob['(' + '<s>' + ',' + key + ')'] = prior_prob[key]\n",
    "total_trans_prob.update(trans_prob)\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump({\"transition\": total_trans_prob, \"emission\": em_prob}, f, ensure_ascii=False, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 3 GREEDY DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for greedy decoding for validation dataset: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Similar to training data extraction\n",
    "validation_data = pd.read_csv(\"./data/dev\", sep = '\\t', names = ['id', 'words', 'pos'])\n",
    "validation_data['occ'] = validation_data.groupby('words')['words'].transform('size')\n",
    "valid_sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for row in validation_data.itertuples():\n",
    "    if(row.id == 1 and first == 0):\n",
    "        valid_sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append((row.words, row.pos))\n",
    "valid_sentences.append(sentence)\n",
    "\"\"\"\n",
    "    greedy decoding\n",
    "\n",
    "        args: trans_prob (dictionary containg transisition probability)\n",
    "              em_prob (dictionary containing emission probability)\n",
    "              prior_prob (dictionary containing T(s1) probability)\n",
    "              valid_sentences (2D list of sentences for testing our HMM model)\n",
    "              tags (list of distince pos tags)\n",
    "        \n",
    "        return: sequences (2D list of pos tag for each sentence we tested)\n",
    "                total_score (2D list of score for all tags in the sequence)\n",
    "        \n",
    "        In greedy decoding for every change in states we are calculating the score\n",
    "        and storing tag which is giving maximum score an using it as previous state.\n",
    "\n",
    "        If a word in testing or validation dataset isn't present in the training dataset\n",
    "        then emission will not have an entry. So to handle that case we are using the\n",
    "        probablity for unknown words.\n",
    "\"\"\"\n",
    "def greedy_decoding(trans_prob, em_prob, prior_prob, valid_sentences, tags):\n",
    "    sequences = []\n",
    "    total_score = []\n",
    "    for sen in valid_sentences:\n",
    "        prev_tag = None\n",
    "        seq = []\n",
    "        score = []\n",
    "        for i in range(len(sen)):\n",
    "            best_score = -1\n",
    "            for j in range(len(tags)):\n",
    "                state_score = 1\n",
    "                if i == 0:\n",
    "                    state_score *= prior_prob[tags[j]]\n",
    "                else:\n",
    "                    if str(\"(\" + prev_tag  + \",\" + tags[j] + \")\") in trans_prob:\n",
    "                        state_score *= trans_prob[\"(\" + prev_tag  + \",\" + tags[j] + \")\"]\n",
    "                \n",
    "                if str(\"(\" + tags[j] + \", \" + sen[i][0] + \")\") in em_prob:\n",
    "                    state_score *= em_prob[\"(\" + tags[j] + \", \" + sen[i][0] + \")\"]\n",
    "                else:\n",
    "                    state_score *= em_prob[\"(\" + tags[j] + \", \" + \"<unk>\" + \")\"]\n",
    "                \n",
    "                if(state_score > best_score):\n",
    "                    best_score = state_score\n",
    "                    highest_prob_tag = tags[j]\n",
    "                    \n",
    "            prev_tag = highest_prob_tag\n",
    "            seq.append(prev_tag)\n",
    "            score.append(best_score)\n",
    "        sequences.append(seq)\n",
    "        total_score.append(score)\n",
    "\n",
    "    return sequences, total_score\n",
    "\n",
    "sequences, total_score = greedy_decoding(trans_prob, em_prob, prior_prob, valid_sentences, tags)\n",
    "# To calculate the accuracy\n",
    "def measure_acc(sequences, valid_sentences):\n",
    "    count = 0\n",
    "    corr_tag_count = 0\n",
    "    for i in range(len(valid_sentences)):\n",
    "        for j in range(len(valid_sentences[i])):\n",
    "\n",
    "            if(sequences[i][j] == valid_sentences[i][j][1]):\n",
    "                corr_tag_count += 1\n",
    "            count +=1\n",
    "    \n",
    "    acc = corr_tag_count / count\n",
    "    return acc\n",
    "\n",
    "print(\"Accuracy for greedy decoding for validation dataset: {:.2f}\".format(measure_acc(sequences, valid_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar extraction for testing dataset as for training dataset\n",
    "test_data = pd.read_csv(\"./data/test\", sep = '\\t', names = ['id', 'words'])\n",
    "test_data['occ'] = test_data.groupby('words')['words'].transform('size')\n",
    "\n",
    "test_sentences = []\n",
    "sentence = []\n",
    "first = 1\n",
    "for row in test_data.itertuples():\n",
    "    if(row.id == 1 and first == 0):\n",
    "        test_sentences.append(sentence)\n",
    "        sentence = []\n",
    "    first = 0\n",
    "    sentence.append(row.words)\n",
    "test_sentences.append(sentence)\n",
    "\n",
    "test_sequences, test_score = greedy_decoding(trans_prob, em_prob, prior_prob, test_sentences, tags)\n",
    "\n",
    "# Generating outfile\n",
    "def output_file(test_inputs, test_outputs, filename):\n",
    "    res = []\n",
    "    for i in range(len(test_inputs)):\n",
    "        s = []\n",
    "        for j in range(len(test_inputs[i])):\n",
    "            s.append((str(j+1), test_inputs[i][j], test_outputs[i][j]))\n",
    "        res.append(s)\n",
    "    \n",
    "    with open(filename + \".out\", 'w') as f:\n",
    "        for ele in res:\n",
    "            f.write(\"\\n\".join([str(item[0]) + \"\\t\" + item[1] + \"\\t\" + item[2] for item in ele]))\n",
    "            f.write(\"\\n\\n\")\n",
    " \n",
    "output_file(test_sentences, test_sequences, \"greedy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 4 VITERBI DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Viterbi Decoding\n",
    "\n",
    "        args: trans_prob (dictionary containg transisition probability)\n",
    "              em_prob (dictionary containing emission probability)\n",
    "              prior_prob (dictionary containing T(s1) probability)\n",
    "              sen (2D list of sentences for testing our HMM model)\n",
    "              tags (list of distince pos tags)\n",
    "        \n",
    "        return: Viterbi List -> A 1D list storing all the scores for the \n",
    "                                previous states pos.\n",
    "                Cache -> A dictionary storing all indides of pos and \"pos\" as a key\n",
    "                         and value as score or cumulative probability\n",
    "                         Dictionary will only make update when for any state we find\n",
    "                         that a transition for one tag to another is better than other\n",
    "                         transition mapping.\n",
    "\"\"\"\n",
    "\n",
    "def viterbi_decoding(trans_prob, em_prob, prior_prob, sen, tags):\n",
    "\n",
    "    n = len(tags)\n",
    "    viterbi_list = []\n",
    "    cache = {}\n",
    "    for si in tags:\n",
    "        if str(\"(\" + si + \", \" + sen[0][0] + \")\") in em_prob:\n",
    "            viterbi_list.append(prior_prob[si] * em_prob[\"(\" + si + \", \" + sen[0][0] + \")\"])\n",
    "        else:\n",
    "            viterbi_list.append(prior_prob[si] * em_prob[\"(\" + si + \", \" + \"<unk>\" + \")\"])\n",
    "\n",
    "    for i, l in enumerate(sen):\n",
    "        word = l[0]\n",
    "        if i == 0: continue\n",
    "        temp_list = [None] * n\n",
    "        for j, tag in enumerate(tags):\n",
    "            score = -1\n",
    "            val = 1\n",
    "            for k, prob in enumerate(viterbi_list):\n",
    "                if str(\"(\" + tags[k] + \",\" + tag + \")\") in trans_prob and str(\"(\" + tag + \", \" + word + \")\") in em_prob:\n",
    "                    val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * em_prob[\"(\" + tag + \", \" + word + \")\"]\n",
    "                else:\n",
    "                   val = prob * trans_prob[\"(\" + tags[k] + \",\" + tag + \")\"] * em_prob[\"(\" + tag + \", \" + \"<unk>\" + \")\"]\n",
    "                if(score < val):\n",
    "                    score = val\n",
    "                    cache[str(i) + \", \" + tag] = [tags[k], val]\n",
    "            temp_list[j] = score\n",
    "        viterbi_list = [x for x in temp_list]\n",
    "    \n",
    "    return cache, viterbi_list\n",
    "\n",
    "\n",
    "c = []\n",
    "v = []\n",
    "\n",
    "for sen in valid_sentences:\n",
    "    a, b = viterbi_decoding(trans_prob, em_prob, prior_prob, sen, tags)\n",
    "    c.append(a)\n",
    "    v.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for viterbi decoding for validation dataset: 0.95\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    After calculating all the best probalilties and storing it into cache dictionary and viterbi list\n",
    "    We are back propogating to generate the sequence of pos tags.\n",
    "    Taking argmax of the viterbi list for the last word propgrating it till we reach to the first word.\n",
    "\n",
    "    best sequence will return a 2D list containing sequence of tags for each sentences in a test data\n",
    "\"\"\"\n",
    "def viterbi_backward(tags, cache, viterbi_list):\n",
    "\n",
    "    num_states = len(tags)\n",
    "    n = len(cache) // num_states\n",
    "    best_sequence = []\n",
    "    best_sequence_breakdown = []\n",
    "    x = tags[np.argmax(np.asarray(viterbi_list))]\n",
    "    best_sequence.append(x)\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        val = cache[str(i) + ', ' + x][1]\n",
    "        x = cache[str(i) + ', ' + x][0]\n",
    "        best_sequence = [x] + best_sequence\n",
    "        best_sequence_breakdown =  [val] + best_sequence_breakdown\n",
    "    \n",
    "    return best_sequence, best_sequence_breakdown\n",
    "\n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for cache, viterbi_list in zip(c, v):\n",
    "\n",
    "    a, b = viterbi_backward(tags, cache, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "print(\"Accuracy for viterbi decoding for validation dataset: {:.2f}\".format(measure_acc(best_seq, valid_sentences)))\n",
    "\n",
    "c = []\n",
    "v = []\n",
    "\n",
    "for sen in test_sentences:\n",
    "    a, b = viterbi_decoding(trans_prob, em_prob, prior_prob, sen, tags)\n",
    "    c.append(a)\n",
    "    v.append(b)\n",
    "\n",
    "best_seq = []\n",
    "best_seq_score = []\n",
    "for cache, viterbi_list in zip(c, v):\n",
    "\n",
    "    a, b = viterbi_backward(tags, cache, viterbi_list)\n",
    "    best_seq.append(a)\n",
    "    best_seq_score.append(b)\n",
    "\n",
    "output_file(test_sentences, best_seq, 'viterbi')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05a30917fcfa4e776f7a314fdfe61b14f33821188b170261aa8594464f55dc4a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
