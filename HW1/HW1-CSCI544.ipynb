{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mishr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../amazon_reviews_us_Kitchen_v1_00.tsv\", sep = \"\\t\", error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEWS SAMPPLE WITH RATING\n",
      "   star_rating                                        review_body\n",
      "0          5.0                Beautiful.  Looks great on counter.\n",
      "1          5.0  I personally have 5 days sets and have also bo...\n",
      "2          5.0  Fabulous and worth every penny. Used for clean...\n",
      "STATISIICS OF THE REVIEWS: \n",
      "rating 1:  426900\n",
      "rating 2:  241948\n",
      "rating 3:  349547\n",
      "rating 4:  731733\n",
      "rating 5:  3124759\n"
     ]
    }
   ],
   "source": [
    "df = df[['star_rating', 'review_body']]\n",
    "print(\"REVIEWS SAMPPLE WITH RATING\")\n",
    "print(df.head(3))\n",
    "print(\"STATISIICS OF THE REVIEWS: \")\n",
    "print(\"rating 1: \", len(df[df.star_rating == 1]))\n",
    "print(\"rating 2: \", len(df[df.star_rating == 2]))\n",
    "print(\"rating 3: \", len(df[df.star_rating == 3]))\n",
    "print(\"rating 4: \", len(df[df.star_rating == 4]))\n",
    "print(\"rating 5: \", len(df[df.star_rating == 5]))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018348 ,  3856296 ,  349539\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df.apply(lambda row: 1 if row.star_rating > 3 else 0, axis = 1)\n",
    "r0 = len(df[df.label == 0])\n",
    "r1 = len(df[df.label == 1])\n",
    "r3 = len(df[df.star_rating == 3])\n",
    "df = df[df.star_rating != 3]\n",
    "print(r0, \", \", r1, \", \", r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = df[df['label'] == 1]\n",
    "negative = df[df['label'] == 0]\n",
    "positive = positive.sample(n = 100000, random_state = 200)\n",
    "negative = negative.sample(n = 100000, random_state=200)\n",
    "dataset = pd.concat([positive, negative])\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "train=dataset.sample(frac=0.8,random_state=200)\n",
    "test = dataset.drop(train.index)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclbdc = (sum(train.review_body.str.len()) + sum(test.review_body.str.len()))/200000\n",
    "review_sample = list(train.review_body.head(3))\n",
    "# convert all the reviews to lowercase\n",
    "train['review_body'] = train['review_body'].str.lower()\n",
    "test['review_body'] = test['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_html_and_url(x):\n",
    "    x = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', x, flags=re.MULTILINE)\n",
    "    soup = BeautifulSoup(x, 'html.parser')\n",
    "    x = soup.get_text()\n",
    "    return x\n",
    "\n",
    "train.review_body = train.review_body.apply(lambda x: remove_html_and_url(x))\n",
    "test.review_body = test.review_body.apply(lambda x: remove_html_and_url(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.review_body = train.review_body.apply(lambda x: re.sub(\"[^a-zA-Z']+\", \" \", x))\n",
    "test.review_body = test.review_body.apply(lambda x: re.sub(\"[^a-zA-Z']+\", \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.review_body = train.review_body.apply(lambda x: re.sub(' +', ' ', x))\n",
    "test.review_body = test.review_body.apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def contractionfunction(s):\n",
    "    s = contractions.fix(s)\n",
    "    s = re.sub(\"[^a-zA-Z]+\", \" \", s)\n",
    "    return s\n",
    "train.review_body = train.review_body.apply(lambda x: contractionfunction(x))\n",
    "test.review_body = test.review_body.apply(lambda x: contractionfunction(x))\n",
    "acladc = (sum(train.review_body.str.len()) + sum(test.review_body.str.len()))/200000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [put, south, facing, window, parts, shades, di...\n",
      "1         [recipient, loved, gift, makes, satisfied, cus...\n",
      "2                     [exactly, expected, ordered, product]\n",
      "3                                                    [love]\n",
      "4         [handy, gadget, monitor, temps, fridge, differ...\n",
      "                                ...                        \n",
      "159995                           [vegetable, peeler, thing]\n",
      "159996    [pans, best, ever, used, gave, little, cooking...\n",
      "159997    [boyfriend, moved, new, apartment, needed, dis...\n",
      "159998                                              [stars]\n",
      "159999    [worked, days, ring, tightens, bottle, snapped...\n",
      "Name: review_body, Length: 160000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "aclbdp = (sum(train.review_body.str.len()) + sum(test.review_body.str.len()))/200000\n",
    "def remove_stopwords(s):\n",
    "    text_tokens = word_tokenize(s)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
    "    return tokens_without_sw\n",
    "train.review_body = train.review_body.apply(remove_stopwords)\n",
    "test.review_body = test.review_body.apply(remove_stopwords)\n",
    "print(train.review_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICS OF THREE CLASES: \n",
      "label 0:  1018348 , label 1:  3856296 , label 3:  349539\n",
      "Average length of reviews before data cleaning:  322.619475\n",
      "Average length of reviews after data cleaning:  301.630745\n",
      "Average length of reviews before data pre-processing:  301.630745\n",
      "Average length of reviews after data pre-processing:  184.227195\n",
      "Sample Reviews before data cleaning \n",
      "\n",
      "I put these on a south facing window and parts of the shades disintigrated over time due to the intense sun where I live - Colorado.  Hunter Douglas would not cover them with their warranty as it states that exposure to the elements voids the warranty.  They consider the sun an element...I guess you need to use these shades where the sun doesn't shine. These are way too expensive to be replacing them every several years.  Just a heads up if you are considering buying these. I won't spend the money for these again. As a follow up- 8/29/07 - I wrote to the corporate office and Hunter Douglas is replacing the Sillouettes under warranty.\n",
      "The recipient loved her gift and that makes for a very satisfied customer.  Thanks.\n",
      "Exactly what I expected when I ordered the product.\n",
      "Sample Reviews after data cleaning and pre-processing:\n",
      "\n",
      "put south facing window part shade disintigrated time due intense sun live colorado hunter douglas would cover warranty state exposure element void warranty consider sun shine way expensive replacing every several year head considering buying spend money follow wrote corporate office hunter douglas replacing sillouettes warranty\n",
      "recipient loved gift make satisfied customer thanks\n",
      "exactly expected ordered product\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(x):\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(word) for word in x]\n",
    "    x = ' '.join(lemmatize_tokens)\n",
    "    return x\n",
    "\n",
    "train.review_body = train.review_body.apply(lemmatize)\n",
    "test.review_body = test.review_body.apply(lemmatize)\n",
    "review_sample_2 = list(train.review_body.head(3))\n",
    "acladp = (sum(train.review_body.str.len()) + sum(test.review_body.str.len()))/200000\n",
    "\n",
    "print(\"STATISTICS OF THREE CLASES: \")\n",
    "print(\"label 0: \", r0, \", label 1: \", r1, \", label 3: \", r3)\n",
    "\n",
    "print(\"Average length of reviews before data cleaning: \", aclbdc)\n",
    "print(\"Average length of reviews after data cleaning: \", acladc)\n",
    "\n",
    "print(\"Average length of reviews before data pre-processing: \", aclbdp)\n",
    "print(\"Average length of reviews after data pre-processing: \", acladp)\n",
    "\n",
    "print(\"Sample Reviews before data cleaning \\n\")\n",
    "for ele in review_sample:\n",
    "    print(ele)\n",
    "print(\"Sample Reviews after data cleaning and pre-processing:\\n\")\n",
    "for ele in review_sample_2:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(min_df = 0.001)\n",
    "X_train = tfidf_vect.fit_transform(train['review_body'])\n",
    "X_train = pd.DataFrame(X_train.toarray(), columns = tfidf_vect.get_feature_names())\n",
    "X_test = tfidf_vect.transform(test['review_body'])\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns = tfidf_vect.get_feature_names())\n",
    "Y_train = train['label']\n",
    "Y_test = test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87 \n",
      "0.84 \n",
      "0.91 \n",
      "0.88\n",
      "0.86 \n",
      "0.84 \n",
      "0.90 \n",
      "0.87\n"
     ]
    }
   ],
   "source": [
    "def metrics(true, pred):\n",
    "    tn, fp, fn, tp = cm(true, pred).ravel()\n",
    "    acc = (tp + tn)/(tn + fp + fn + tp)\n",
    "    prec = tp/(tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f1 = 2*(rec * prec) / (rec + prec)\n",
    "    return [acc, prec, rec, f1]\n",
    "\n",
    "def print_seq(score_list):\n",
    "    print(\"%.2f\" % score_list[0], \"\\n%.2f\" % score_list[1], \"\\n%.2f\" % score_list[2], \"\\n%.2f\" % score_list[3])\n",
    "\n",
    "percept = Perceptron(max_iter = 80, tol = 1e-5, random_state = 200, eta0 = 0.01)\n",
    "percept.fit(X_train, Y_train)\n",
    "Y_train_pred = percept.predict(X_train)\n",
    "train_score = metrics(Y_train, Y_train_pred)\n",
    "\n",
    "# Predicting on test dataset\n",
    "Y_test_pred = percept.predict(X_test)\n",
    "test_score = metrics(Y_test, Y_test_pred)\n",
    "# print(train_score)\n",
    "# print(test_score)\n",
    "# print(\"TRAIN_SCORES:\")\n",
    "print_seq(train_score)\n",
    "# print(\"TEST_SCORES:\")\n",
    "print_seq(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90 \n",
      "0.90 \n",
      "0.89 \n",
      "0.90\n",
      "0.89 \n",
      "0.90 \n",
      "0.88 \n",
      "0.89\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(max_iter = 5000)\n",
    "model_svc = svc.fit(X_train, Y_train)\n",
    "pred_train_svm = model_svc.predict(X_train)\n",
    "pred_test_svm = model_svc.predict(X_test)\n",
    "train_svm_score = metrics(Y_train, pred_train_svm)\n",
    "test_svm_score = metrics(Y_test, pred_test_svm)\n",
    "\n",
    "# print(train_svm_score)\n",
    "# print(test_svm_score)\n",
    "# print(\"TRAIN_SCORES:\")\n",
    "print_seq(train_svm_score)\n",
    "# print(\"TEST_SCORES:\")\n",
    "print_seq(test_svm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90 \n",
      "0.90 \n",
      "0.89 \n",
      "0.90\n",
      "0.89 \n",
      "0.90 \n",
      "0.88 \n",
      "0.89\n"
     ]
    }
   ],
   "source": [
    "log_reg = LR(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "model = log_reg.fit(X_train, Y_train)\n",
    "pred_train_LR = model.predict(X_train)\n",
    "pred_test_LR = model.predict(X_test)\n",
    "train_LR_score = metrics(Y_train, pred_train_LR)\n",
    "test_LR_score = metrics(Y_test, pred_test_LR)\n",
    "\n",
    "# print(train_LR_score)\n",
    "# print(test_LR_score)\n",
    "# print(\"TRAIN_SCORES:\")\n",
    "print_seq(train_LR_score)\n",
    "# print(\"TEST_SCORES:\")\n",
    "print_seq(test_LR_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87 \n",
      "0.86 \n",
      "0.87 \n",
      "0.87\n",
      "0.86 \n",
      "0.86 \n",
      "0.86 \n",
      "0.86\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "pred_train_MNB = MNB.predict(X_train)\n",
    "pred_test_MNB = MNB.predict(X_test)\n",
    "train_MNB_score = metrics(Y_train, pred_train_MNB)\n",
    "test_MNB_score = metrics(Y_test, pred_test_MNB)\n",
    "\n",
    "# print(train_MNB_score)\n",
    "# print(test_MNB_score)\n",
    "# print(\"TRAIN_SCORES:\")\n",
    "print_seq(train_MNB_score)\n",
    "# print(\"TEST_SCORES:\")\n",
    "print_seq(test_MNB_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
