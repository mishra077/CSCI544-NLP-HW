{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import json\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading Vocabulary and Labels Json File\"\"\"\n",
    "with open('./vocab.json') as json_file:\n",
    "    word_idx = json.load(json_file)\n",
    "\n",
    "with open('./label.json') as json_file:\n",
    "    label_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepraring the dataset\"\"\"\n",
    "def prep_dataset(dataset):\n",
    "    train_x, train_y = list(), list()\n",
    "    x, y = list(), list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "            x = list()\n",
    "            y = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "        y.append(row.NER)\n",
    "\n",
    "    train_x.append(x)\n",
    "    train_y.append(y)\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\"\"\"Read File\"\"\"\n",
    "def read_file(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 2:\n",
    "                id, word, ner_tag = line.strip().split(\" \")\n",
    "                train_df.append([id, word, ner_tag])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word', 'NER'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x, train_y = prep_dataset(train_df)\n",
    "    return train_x, train_y\n",
    "\n",
    "\"\"\" Perparing the testing dataset \"\"\"\n",
    "def prep_dataset_test(dataset):\n",
    "    train_x = list()\n",
    "    x = list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            x = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "\n",
    "    train_x.append(x)\n",
    "    return train_x\n",
    "\n",
    "\"\"\" Read test file \"\"\"\n",
    "def read_file_test(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                id, word = line.strip().split(\" \")\n",
    "                train_df.append([id, word])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x = prep_dataset_test(train_df)\n",
    "    return train_x\n",
    "\n",
    "\n",
    "train_x, train_y = read_file('./data/train')\n",
    "val_x, val_y = read_file('./data/dev')\n",
    "test_x = read_file_test('./data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" For Task - 1 \"\"\"\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions,\n",
    "                            linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred\n",
    "\"\"\" For Task - 2 \"\"\"\n",
    "class BiLSTM_Glove(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size, emb_matrix):\n",
    "        super(BiLSTM_Glove, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred\n",
    "\n",
    "\"\"\" Dataloader Training and Validation \"\"\"\n",
    "class BiLSTM_DataLoader(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])\n",
    "        y_instance = torch.tensor(self.y[index])\n",
    "        return x_instance, y_instance\n",
    "\n",
    "\"\"\" Dataloader for Testing \"\"\"\n",
    "class BiLSTM_TestLoader(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])\n",
    "        return x_instance\n",
    "\n",
    "\"\"\" Collator Class for Training and Validation Dataset\"\"\"\n",
    "class CustomCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        (xx, yy) = zip(*batch)\n",
    "        x_len = [len(x) for x in xx]\n",
    "        y_len = [len(y) for y in yy]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "            batch_labels[j][:cur_len] = yy[j]\n",
    "\n",
    "        batch_data, batch_labels = torch.LongTensor(\n",
    "            batch_data), torch.LongTensor(batch_labels)\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        return batch_data, batch_labels, x_len, y_len\n",
    "\n",
    "\"\"\" Collator Class for Testing Dataset\"\"\"\n",
    "class CustomTestCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        xx = batch\n",
    "        x_len = [len(x) for x in xx]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "\n",
    "        batch_data = torch.LongTensor(batch_data)\n",
    "        batch_data = Variable(batch_data)\n",
    "\n",
    "        return batch_data, x_len\n",
    "\n",
    "\"\"\" Create Embedding Matrix from GLOVE word embedding \"\"\"\n",
    "def create_emb_matrix(word_idx, emb_dict, dimension):\n",
    "\n",
    "    emb_matrix = np.zeros((len(word_idx), dimension))\n",
    "    for word, idx in word_idx.items():\n",
    "        if word in emb_dict:\n",
    "            emb_matrix[idx] = emb_dict[word]\n",
    "        else:\n",
    "            if word.lower() in emb_dict:\n",
    "                emb_matrix[idx] = emb_dict[word.lower()] + 5e-3\n",
    "            else:\n",
    "                emb_matrix[idx] = emb_dict[\"<UNK>\"]\n",
    "\n",
    "    return emb_matrix\n",
    "    \n",
    "    \n",
    "\"\"\" Prepare Vocabulary \"\"\"\n",
    "def prep_vocab(dataset):\n",
    "\n",
    "    vocab = set()\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "\"\"\" Create Vocabulary Dictionary \"\"\"\n",
    "def prep_word_index(train_x, val_x, test_x):\n",
    "\n",
    "    word_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx = 2\n",
    "\n",
    "    for data in [train_x, val_x, test_x]:\n",
    "        for sent in data:\n",
    "            for word in sent:\n",
    "                if word not in word_idx:\n",
    "                    word_idx[word] = idx\n",
    "                    idx += 1\n",
    "    return word_idx\n",
    "\n",
    "\"\"\" Words to Index Conversion \"\"\"\n",
    "def vectorizing_sent(train_x, word_idx):\n",
    "\n",
    "    train_x_vec = list()\n",
    "    tmp_x = list()\n",
    "    for words in train_x:\n",
    "        for word in words:\n",
    "            tmp_x.append(word_idx[word])\n",
    "        train_x_vec.append(tmp_x)\n",
    "        tmp_x = list()\n",
    "\n",
    "    return train_x_vec\n",
    "\n",
    "\"\"\" Create NER - Tags Dictionary \"\"\"\n",
    "def prep_label_dict(train_y, val_y):\n",
    "\n",
    "    label1 = prep_vocab(train_y)\n",
    "    label2 = prep_vocab(val_y)\n",
    "    label = label1.union(label2)\n",
    "    label_tuples = []\n",
    "    counter = 0\n",
    "    for tags in label:\n",
    "        label_tuples.append((tags, counter))\n",
    "        counter += 1\n",
    "    label_dict = dict(label_tuples)\n",
    "    return label_dict\n",
    "\n",
    "\"\"\" Tags to Index Conversion \"\"\"\n",
    "def vectorizing_label(train_y, label_dict):\n",
    "\n",
    "    train_y_vec = list()\n",
    "    for tags in train_y:\n",
    "        tmp_yy = list()\n",
    "        for label in tags:\n",
    "            tmp_yy.append(label_dict[label])\n",
    "        train_y_vec.append(tmp_yy)\n",
    "    return train_y_vec\n",
    "\n",
    "train_x_vec = vectorizing_sent(train_x, word_idx)\n",
    "test_x_vec = vectorizing_sent(test_x, word_idx)\n",
    "val_x_vec = vectorizing_sent(val_x, word_idx)\n",
    "train_y_vec = vectorizing_label(train_y, label_dict)\n",
    "val_y_vec = vectorizing_label(val_y, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_class_weights(label_dict, train_y, val_y):\n",
    "    class_weights = dict()\n",
    "    for key in label_dict:\n",
    "        class_weights[key] = 0\n",
    "    total_nm_tags = 0\n",
    "    for data in [train_y, val_y]:\n",
    "        for tags in data:\n",
    "            for tag in tags:\n",
    "                total_nm_tags += 1\n",
    "                class_weights[tag] += 1\n",
    "\n",
    "    class_wt = list()\n",
    "    for key in class_weights.keys():\n",
    "        if class_weights[key]:\n",
    "            score = round(math.log(0.35*total_nm_tags / class_weights[key]), 2)\n",
    "            class_weights[key] = score if score > 1.0 else 1.0\n",
    "        else:\n",
    "            class_weights[key] = 1.0\n",
    "        class_wt.append(class_weights[key])\n",
    "    class_wt = torch.tensor(class_wt)\n",
    "    return class_wt\n",
    "\n",
    "\n",
    "class_wt = initialize_class_weights(label_dict, train_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_model = BiLSTM(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      bidirectional=True,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict))\n",
    "BiLSTM_model.load_state_dict(torch.load(\"./blstm1.pt\"))\n",
    "BiLSTM_model.to(device)\n",
    "\n",
    "\"\"\"Testing on Validation Dataset \"\"\"\n",
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "\n",
    "# Reverse vocab and label Dictionary                            \n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "\n",
    "file = open(\"./dev1.out\", 'w')\n",
    "for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "    pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    dev_data = dev_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(label), -1))\n",
    "\n",
    "    for i in range(len(dev_data)):\n",
    "        for j in range(len(dev_data[i])):\n",
    "            if dev_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[dev_data[i][j]]\n",
    "                gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n",
    "!perl conll03eval.txt < dev1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing on Testing Dataset \"\"\"\n",
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "\n",
    "\n",
    "file = open(\"test1.out\", 'w')\n",
    "for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "    pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    test_data = test_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(test_data), -1))\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        for j in range(len(test_data[i])):\n",
    "            if test_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[test_data[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, op]))\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "        file.write(\"\\n\") \n",
    "file.close()\n",
    "!perl conll03eval.txt < test1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading Embedding Matrix\"\"\"\n",
    "emb_matrix = np.load('emb_matrix.npy')\n",
    "vocab_size = emb_matrix.shape[0]\n",
    "vector_size = emb_matrix.shape[1]\n",
    "\n",
    "BiLSTM_model = BiLSTM_Glove(vocab_size=len(word_idx),\n",
    "                    embedding_dim=100,\n",
    "                    linear_out_dim=128,\n",
    "                    hidden_dim=256,\n",
    "                    lstm_layers=1,\n",
    "                    bidirectional=True,\n",
    "                    dropout_val=0.33,\n",
    "                    tag_size=len(label_dict),\n",
    "                    emb_matrix = emb_matrix)\n",
    "\n",
    "BiLSTM_model.load_state_dict(torch.load(\"./blstm2.pt\"))\n",
    "BiLSTM_model.to(device)\n",
    "\n",
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "\n",
    "file = open(\"dev2.out\", 'w')\n",
    "for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "    pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    label = label.detach().numpy()\n",
    "    dev_data = dev_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(label), -1))\n",
    "\n",
    "    for i in range(len(dev_data)):\n",
    "        for j in range(len(dev_data[i])):\n",
    "            if dev_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[dev_data[i][j]]\n",
    "                gold = rev_label_dict[label[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n",
    "!perl conll03eval.txt < dev2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing on Testing Dataset \"\"\"\n",
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "\n",
    "rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "file = open(\"test2.out\", 'w')\n",
    "for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "    pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "    pred = pred.cpu()\n",
    "    pred = pred.detach().numpy()\n",
    "    test_data = test_data.detach().numpy()\n",
    "    pred = np.argmax(pred, axis=2)\n",
    "    pred = pred.reshape((len(test_data), -1))\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        for j in range(len(test_data[i])):\n",
    "            if test_data[i][j] != 0:\n",
    "                word = rev_vocab_dict[test_data[i][j]]\n",
    "                op = rev_label_dict[pred[i][j]]\n",
    "                file.write(\" \".join([str(j + 1), word, op]))\n",
    "                file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "file.close()\n",
    "!perl conll03eval.txt < test2.out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
