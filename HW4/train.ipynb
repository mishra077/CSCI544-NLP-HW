{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import json\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(dataset):\n",
    "    train_x, train_y = list(), list()\n",
    "    x, y = list(), list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        # print(type(row.id))\n",
    "        # break\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "            x = list()\n",
    "            y = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "        y.append(row.NER)\n",
    "\n",
    "    train_x.append(x)\n",
    "    train_y.append(y)\n",
    "\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def read_file(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 2:\n",
    "                id, word, ner_tag = line.strip().split(\" \")\n",
    "                train_df.append([id, word, ner_tag])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word', 'NER'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x, train_y = prep_dataset(train_df)\n",
    "    return train_x, train_y\n",
    "\n",
    "\n",
    "def prep_dataset_test(dataset):\n",
    "    train_x = list()\n",
    "    x = list()\n",
    "    first = 1\n",
    "    for row in dataset.itertuples():\n",
    "        # print(type(row.id))\n",
    "        # break\n",
    "        if(row.id == '1' and first == 0):\n",
    "            train_x.append(x)\n",
    "            x = list()\n",
    "        first = 0\n",
    "        x.append(row.word)\n",
    "\n",
    "    train_x.append(x)\n",
    "    return train_x\n",
    "\n",
    "\n",
    "def read_file_test(path):\n",
    "    train_df = list()\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line) > 1:\n",
    "                id, word = line.strip().split(\" \")\n",
    "                train_df.append([id, word])\n",
    "\n",
    "    train_df = pd.DataFrame(train_df, columns=['id', 'word'])\n",
    "    train_df = train_df.dropna()\n",
    "    train_x = prep_dataset_test(train_df)\n",
    "    return train_x\n",
    "\n",
    "\n",
    "train_x, train_y = read_file('./data/train')\n",
    "val_x, val_y = read_file('./data/dev')\n",
    "test_x = read_file_test('./data/test')\n",
    "\n",
    "print(len(train_x), len(train_y))\n",
    "print(len(val_x), len(val_y))\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight.data.uniform_(-1,1)\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions,\n",
    "                            linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred\n",
    "\n",
    "class BiLSTM_Glove(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,\n",
    "                 bidirectional, dropout_val, tag_size, emb_matrix):\n",
    "        super(BiLSTM_Glove, self).__init__()\n",
    "        \"\"\" Hyper Parameters \"\"\"\n",
    "        self.hidden_dim = hidden_dim  # hidden_dim = 256\n",
    "        self.lstm_layers = lstm_layers  # LSTM Layers = 1\n",
    "        self.embedding_dim = embedding_dim  # Embedding Dimension = 100\n",
    "        self.linear_out_dim = linear_out_dim  # Linear Ouput Dimension = 128\n",
    "        self.tag_size = tag_size  # Tag Size = 9\n",
    "        self.emb_matrix = emb_matrix\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        \"\"\" Initializing Network \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding Layer\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))\n",
    "        self.LSTM = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)  # 2 for bidirection\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.elu = nn.ELU(alpha=0.01)\n",
    "        self.classifier = nn.Linear(linear_out_dim, self.tag_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.lstm_layers * self.num_directions,\n",
    "                            batch_size, self.hidden_dim).to(device))\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, sen, sen_len):  # sen_len\n",
    "        # Set initial states\n",
    "        batch_size = sen.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        embedded = self.embedding(sen).float()\n",
    "        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.LSTM(packed_embedded, (h_0, c_0))\n",
    "        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        dropout = self.dropout(output_unpacked)\n",
    "        lin = self.fc(dropout)\n",
    "        pred = self.elu(lin)\n",
    "        pred = self.classifier(pred)\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "class BiLSTM_DataLoader(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance, y_instance\n",
    "\n",
    "class BiLSTM_TestLoader(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_instance = torch.tensor(self.x[index])  # , dtype=torch.long\n",
    "        # y_instance = torch.tensor(self.y[index])  # , dtype=torch.float\n",
    "        return x_instance\n",
    "\n",
    "\n",
    "class CustomCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        (xx, yy) = zip(*batch)\n",
    "        x_len = [len(x) for x in xx]\n",
    "        y_len = [len(y) for y in yy]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "            batch_labels[j][:cur_len] = yy[j]\n",
    "\n",
    "        batch_data, batch_labels = torch.LongTensor(\n",
    "            batch_data), torch.LongTensor(batch_labels)\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "\n",
    "        return batch_data, batch_labels, x_len, y_len\n",
    "\n",
    "class CustomTestCollator(object):\n",
    "\n",
    "    def __init__(self, vocab, label):\n",
    "        self.params = vocab\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        xx = batch\n",
    "        x_len = [len(x) for x in xx]\n",
    "        # y_len = [len(y) for y in yy]\n",
    "        batch_max_len = max([len(s) for s in xx])\n",
    "        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))\n",
    "        # batch_labels = -1*np.zeros((len(xx), batch_max_len))\n",
    "        for j in range(len(xx)):\n",
    "            cur_len = len(xx[j])\n",
    "            batch_data[j][:cur_len] = xx[j]\n",
    "            # batch_labels[j][:cur_len] = yy[j]\n",
    "\n",
    "        batch_data = torch.LongTensor(batch_data)\n",
    "        batch_data = Variable(batch_data)\n",
    "\n",
    "        return batch_data, x_len\n",
    "\n",
    "def create_emb_matrix(word_idx, emb_dict, dimension):\n",
    "\n",
    "    emb_matrix = np.zeros((len(word_idx), dimension))\n",
    "    for word, idx in word_idx.items():\n",
    "        if word in emb_dict:\n",
    "            emb_matrix[idx] = emb_dict[word]\n",
    "        else:\n",
    "            if word.lower() in emb_dict:\n",
    "                emb_matrix[idx] = emb_dict[word.lower()] + 5e-3\n",
    "            else:\n",
    "                emb_matrix[idx] = emb_dict[\"<UNK>\"]\n",
    "\n",
    "    return emb_matrix\n",
    "    \n",
    "    \n",
    "\"\"\" Prepare Vocabulary\"\"\"\n",
    "def prep_vocab(dataset):\n",
    "\n",
    "    vocab = set()\n",
    "    for sentence in dataset:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def prep_word_index(train_x, val_x, test_x):\n",
    "\n",
    "    word_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    idx = 2\n",
    "\n",
    "    for data in [train_x, val_x, test_x]:\n",
    "        for sent in data:\n",
    "            for word in sent:\n",
    "                if word not in word_idx:\n",
    "                    word_idx[word] = idx\n",
    "                    idx += 1\n",
    "    return word_idx\n",
    "\n",
    "\n",
    "def vectorizing_sent(train_x, word_idx):\n",
    "\n",
    "    train_x_vec = list()\n",
    "    tmp_x = list()\n",
    "    for words in train_x:\n",
    "        for word in words:\n",
    "            tmp_x.append(word_idx[word])\n",
    "        train_x_vec.append(tmp_x)\n",
    "        tmp_x = list()\n",
    "\n",
    "    return train_x_vec\n",
    "\n",
    "\n",
    "def prep_label_dict(train_y, val_y):\n",
    "\n",
    "    label1 = prep_vocab(train_y)\n",
    "    label2 = prep_vocab(val_y)\n",
    "    label = label1.union(label2)\n",
    "    label_tuples = []\n",
    "    counter = 0\n",
    "    for tags in label:\n",
    "        label_tuples.append((tags, counter))\n",
    "        counter += 1\n",
    "    label_dict = dict(label_tuples)\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def vectorizing_label(train_y, label_dict):\n",
    "\n",
    "    train_y_vec = list()\n",
    "    for tags in train_y:\n",
    "        tmp_yy = list()\n",
    "        for label in tags:\n",
    "            tmp_yy.append(label_dict[label])\n",
    "        train_y_vec.append(tmp_yy)\n",
    "    return train_y_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = prep_word_index(train_x, val_x, test_x)\n",
    "train_x_vec = vectorizing_sent(train_x, word_idx)\n",
    "test_x_vec = vectorizing_sent(test_x, word_idx)\n",
    "val_x_vec = vectorizing_sent(val_x, word_idx)\n",
    "label_dict = prep_label_dict(train_y, val_y)\n",
    "train_y_vec = vectorizing_label(train_y, label_dict)\n",
    "val_y_vec = vectorizing_label(val_y, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_class_weights(label_dict, train_y, val_y):\n",
    "    class_weights = dict()\n",
    "    for key in label_dict:\n",
    "        class_weights[key] = 0\n",
    "    total_nm_tags = 0\n",
    "    for data in [train_y, val_y]:\n",
    "        for tags in data:\n",
    "            for tag in tags:\n",
    "                total_nm_tags += 1\n",
    "                class_weights[tag] += 1\n",
    "\n",
    "    class_wt = list()\n",
    "    for key in class_weights.keys():\n",
    "        if class_weights[key]:\n",
    "            score = round(math.log(0.35*total_nm_tags / class_weights[key]), 2)\n",
    "            class_weights[key] = score if score > 1.0 else 1.0\n",
    "        else:\n",
    "            class_weights[key] = 1.0\n",
    "        class_wt.append(class_weights[key])\n",
    "    class_wt = torch.tensor(class_wt)\n",
    "    return class_wt\n",
    "\n",
    "\n",
    "class_wt = initialize_class_weights(label_dict, train_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_model = BiLSTM(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      bidirectional=True,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict))\n",
    "BiLSTM_model.to(device)\n",
    "print(BiLSTM_model)\n",
    "\n",
    "BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader = DataLoader(dataset=BiLSTM_train,\n",
    "                        batch_size=4,\n",
    "                        drop_last=True,\n",
    "                        collate_fn=custom_collator)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
    "epochs = 200\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = BiLSTM_model(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "    torch.save(BiLSTM_model.state_dict(),\n",
    "               'BiLSTM_b1_epoch_' + str(i) + '.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "for e in range(1,epochs + 1):\n",
    "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))#125\n",
    "    BiLSTM_model.to(device)\n",
    "\n",
    "    \n",
    "    print(label_dict)\n",
    "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "    file = open(\"./dev1.out\", 'w')\n",
    "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "        pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        label = label.detach().numpy()\n",
    "        dev_data = dev_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(label), -1))\n",
    "\n",
    "        for i in range(len(dev_data)):\n",
    "            for j in range(len(dev_data[i])):\n",
    "                if dev_data[i][j] != 0:\n",
    "                    word = rev_vocab_dict[dev_data[i][j]]\n",
    "                    gold = rev_label_dict[label[i][j]]\n",
    "                    op = rev_label_dict[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                    file.write(\"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    file.close()\n",
    "    !perl conll03eval.txt < dev1.out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing on Testing Dataset \"\"\"\n",
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "for e in range(1,epochs + 1):\n",
    "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_b1_epoch_\"+str(e)+\".pt\"))#125\n",
    "    BiLSTM_model.to(device)\n",
    "\n",
    "    \n",
    "    print(label_dict)\n",
    "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "    file = open(\"test1.out\", 'w')\n",
    "    for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "        pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        test_data = test_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(test_data), -1))\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            for j in range(len(test_data[i])):\n",
    "                if test_data[i][j] != 0:\n",
    "                    word = rev_vocab_dict[test_data[i][j]]\n",
    "                    op = rev_label_dict[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, op]))\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "            file.write(\"\\n\")        \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK -2\n",
    "glove = pd.read_csv('./glove.6B.100d.txt', sep=\" \",\n",
    "                    quoting=3, header=None, index_col=0)\n",
    "glove_emb = {key: val.values for key, val in glove.T.items()}\n",
    "\n",
    "word_idx = prep_word_index(train_x, val_x, test_x)\n",
    "glove_vec = np.array([glove_emb[key] for key in glove_emb])\n",
    "glove_emb[\"<PAD>\"] = np.zeros((100,), dtype=\"float64\")\n",
    "glove_emb[\"<UNK>\"] = np.mean(glove_vec, axis=0, keepdims=True).reshape(100,)\n",
    "emb_matrix = create_emb_matrix(\n",
    "    word_idx=word_idx, emb_dict=glove_emb, dimension=100)\n",
    "\n",
    "vocab_size = emb_matrix.shape[0]\n",
    "vector_size = emb_matrix.shape[1]\n",
    "print(vocab_size, vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_model = BiLSTM_Glove(vocab_size=len(word_idx),\n",
    "                      embedding_dim=100,\n",
    "                      linear_out_dim=128,\n",
    "                      hidden_dim=256,\n",
    "                      lstm_layers=1,\n",
    "                      bidirectional=True,\n",
    "                      dropout_val=0.33,\n",
    "                      tag_size=len(label_dict),\n",
    "                      emb_matrix=emb_matrix)\n",
    "BiLSTM_model.to(device)\n",
    "print(BiLSTM_model)\n",
    "\n",
    "BiLSTM_train = BiLSTM_DataLoader(train_x_vec, train_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader = DataLoader(dataset=BiLSTM_train,\n",
    "                        batch_size=8,\n",
    "                        drop_last=True,\n",
    "                        collate_fn=custom_collator)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_wt)\n",
    "criterion = criterion.to(device)\n",
    "criterion.requres_grad = True\n",
    "optimizer = torch.optim.SGD(BiLSTM_model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=15, gamma=0.9)\n",
    "epochs = 50\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    train_loss = 0.0\n",
    "    for input, label, input_len, label_len in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = BiLSTM_model(input.to(device), input_len)\n",
    "        output = output.view(-1, len(label_dict))\n",
    "        label = label.view(-1)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * input.size(1)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(i, train_loss))\n",
    "    torch.save(BiLSTM_model.state_dict(),\n",
    "               'BiLSTM_glove_' + str(i) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting for validation dataset\n",
    "BiLSTM_dev = BiLSTM_DataLoader(val_x_vec, val_y_vec)\n",
    "custom_collator = CustomCollator(word_idx, label_dict)\n",
    "dataloader_dev = DataLoader(dataset=BiLSTM_dev,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            collate_fn=custom_collator)\n",
    "for e in range(1, 51):\n",
    "    BiLSTM_model = BiLSTM_Glove(vocab_size=len(word_idx),\n",
    "                        embedding_dim=100,\n",
    "                        linear_out_dim=128,\n",
    "                        hidden_dim=256,\n",
    "                        lstm_layers=1,\n",
    "                        bidirectional=True,\n",
    "                        dropout_val=0.33,\n",
    "                        tag_size=len(label_dict),\n",
    "                        emb_matrix = emb_matrix)\n",
    "\n",
    "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_glove_\"+str(e)+\".pt\"))#125\n",
    "    BiLSTM_model.to(device)\n",
    "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "    \n",
    "    file = open(\"dev2.out\", 'w')\n",
    "    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:\n",
    "\n",
    "        pred = BiLSTM_model(dev_data.to(device), dev_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        label = label.detach().numpy()\n",
    "        dev_data = dev_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(label), -1))\n",
    "\n",
    "        for i in range(len(dev_data)):\n",
    "            for j in range(len(dev_data[i])):\n",
    "                if dev_data[i][j] != 0:\n",
    "                    word = rev_vocab_dict[dev_data[i][j]]\n",
    "                    gold = rev_label_dict[label[i][j]]\n",
    "                    op = rev_label_dict[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, gold, op]))\n",
    "                    file.write(\"\\n\")\n",
    "            file.write(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    !perl conll03eval.txt < dev2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM_test = BiLSTM_TestLoader(test_x_vec)\n",
    "custom_test_collator = CustomTestCollator(word_idx, label_dict)\n",
    "dataloader_test = DataLoader(dataset=BiLSTM_test,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                drop_last=True,\n",
    "                                collate_fn=custom_test_collator)\n",
    "for e in range(1,epochs + 1):\n",
    "    BiLSTM_model.load_state_dict(torch.load(\"./BiLSTM_glove_\"+str(e)+\".pt\"))#125\n",
    "    BiLSTM_model.to(device)\n",
    "    rev_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    rev_vocab_dict = {v: k for k, v in word_idx.items()}\n",
    "\n",
    "    file = open(\"test1.out\", 'w')\n",
    "    for test_data, test_data_len in dataloader_test:\n",
    "\n",
    "        pred = BiLSTM_model(test_data.to(device), test_data_len)\n",
    "        pred = pred.cpu()\n",
    "        pred = pred.detach().numpy()\n",
    "        test_data = test_data.detach().numpy()\n",
    "        pred = np.argmax(pred, axis=2)\n",
    "        pred = pred.reshape((len(test_data), -1))\n",
    "        \n",
    "        for i in range(len(test_data)):\n",
    "            for j in range(len(test_data[i])):\n",
    "                if test_data[i][j] != 0:\n",
    "                    word = rev_vocab_dict[test_data[i][j]]\n",
    "                    op = rev_label_dict[pred[i][j]]\n",
    "                    file.write(\" \".join([str(j+1), word, op]))\n",
    "                    file.write(\"\\n\")\n",
    "\n",
    "            file.write(\"\\n\")        \n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
